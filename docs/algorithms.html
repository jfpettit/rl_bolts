---

title: algorithms

keywords: fastai
sidebar: home_sidebar

summary: "This module contains algorithms we choose to implement and test."
description: "This module contains algorithms we choose to implement and test."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/07_algorithms.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PPO" class="doc_header"><code>class</code> <code>PPO</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/algorithms.py#L23" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PPO</code>(<strong><code>env</code></strong>:<code>str</code>, <strong><code>hidden_sizes</code></strong>:<code>Optional</code>[<code>tuple</code>]=<em><code>(32, 32)</code></em>, <strong><code>gamma</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.99</code></em>, <strong><code>lam</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.97</code></em>, <strong><code>clipratio</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.2</code></em>, <strong><code>train_iters</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>80</code></em>, <strong><code>batch_size</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>4000</code></em>, <strong><code>pol_lr</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.0003</code></em>, <strong><code>val_lr</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.001</code></em>, <strong><code>maxkl</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.01</code></em>, <strong><code>seed</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>0</code></em>) :: <code>LightningModule</code></p>
</blockquote>
<p>Implementation of the Proximal Policy Optimization (PPO) algorithm. See the paper: <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
<p>It is a PyTorch Lightning Module. See their docs: <a href="https://pytorch-lightning.readthedocs.io/en/latest/">https://pytorch-lightning.readthedocs.io/en/latest/</a></p>
<p>Args:</p>
<ul>
<li>env (str): Environment to run in. Handles vector observation environments with either gym.spaces.Box or gym.spaces.Discrete
action space.</li>
<li>hidden_sizes (tuple): Hidden layer sizes for actor-critic network.</li>
<li>gamma (float): Discount factor.</li>
<li>lam (float): Lambda factor for GAE-Lambda calculation.</li>
<li>clipratio (float): Clip ratio for PPO-clip objective.</li>
<li>train_iters (int): How many steps to take over the latest data batch.</li>
<li>batch_size (int): How many interactions to collect per update.</li>
<li>pol_lr (float): Learning rate for the policy optimizer.</li>
<li>val_lr (float): Learning rate for the value optimizer.</li>
<li>maxkl (float): Max allowed KL divergence between policy updates.</li>
<li>seed (int): Random seed for pytorch and numpy</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is an example training PPO on the <a href="https://gym.openai.com/envs/CartPole-v1/">CartPole-v1 environment</a>. Since it is a <a href="https://pytorch-lightning.readthedocs.io/en/latest/">PyTorch-Lightning Module</a> it is trained using their Trainer API.</p>
<p><strong>Note that this PPO implementation needs to be more thoroughly benchmarked and so may be a work in progress.</strong></p>
<p>The <code>reload_dataloaders_every_epoch</code> flag is needed to ensure that during each training step, the updates are computed on the latest batch of data.</p>
<p>To see how we implement this, view the source code for the <a href="/rl_bolts/algorithms#PPO"><code>PPO</code></a> class.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">reload_dataloaders_every_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

