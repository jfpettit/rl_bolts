---

title: env_wrappers

keywords: fastai
sidebar: home_sidebar

summary: "Here we provide a useful set of environment wrappers."
description: "Here we provide a useful set of environment wrappers."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/05_env_wrappers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ToTorchWrapper" class="doc_header"><code>class</code> <code>ToTorchWrapper</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L12" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ToTorchWrapper</code>(<strong><code>env</code></strong>:<code>Env</code>) :: <code>Wrapper</code></p>
</blockquote>
<p>Environment wrapper for converting actions from torch.Tensors to np.array and converting observations from np.array to
torch.Tensors.</p>
<p>Args:</p>
<ul>
<li>env (gym.Env): Environment to wrap. Should be a subclass of gym.Env and follow the OpenAI Gym API.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ToTorchWrapper.reset" class="doc_header"><code>ToTorchWrapper.reset</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L25" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ToTorchWrapper.reset</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Reset the environment.</p>
<p>Returns:</p>
<ul>
<li>tensor_obs (torch.Tensor): output of reset as PyTorch Tensor.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ToTorchWrapper.step" class="doc_header"><code>ToTorchWrapper.step</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L36" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ToTorchWrapper.step</code>(<strong><code>action</code></strong>:<code>Tensor</code>, <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Execute environment step.</p>
<p>Converts from torch.Tensor action and returns observations as a torch.Tensor.</p>
<p>Returns:</p>
<ul>
<li>tensor_obs (torch.Tensor): Next observations as pytorch tensor.</li>
<li>reward (float or int): The reward earned at the current timestep.</li>
<li>done (bool): Whether the episode is in a terminal state.</li>
<li>infos (dict): The info dict from the environment.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ToTorchWrapper.action2np" class="doc_header"><code>ToTorchWrapper.action2np</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L54" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ToTorchWrapper.action2np</code>(<strong><code>action</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Convert torch.Tensor action to NumPy.</p>
<p>Args:</p>
<ul>
<li>action (torch.Tensor): The action to convert.</li>
</ul>
<p>Returns:</p>
<ul>
<li>np_act (np.array or int): The action converted to numpy.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Example usage of the <a href="/rl_bolts/env_wrappers#ToTorchWrapper"><code>ToTorchWrapper</code></a> is demonstrated below.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ToTorchWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial obs:&quot;</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="c1"># need to convert action to PyTorch Tensor because ToTorchWrapper expects actions as Tensors.</span>
<span class="c1"># normally you would not need to do this, your PyTorch NN actor will output a Tensor by default.</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">stepped</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;stepped once:&quot;</span><span class="p">,</span> <span class="n">stepped</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Entering interaction loop! </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># interaction loop</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random policy got </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> reward!&quot;</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">99</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting new episode.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interaction loop ended! Got reward </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> before episode was cut off.&quot;</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>initial obs: tensor([0.0359, 0.0385, 0.0408, 0.0239])
stepped once: (tensor([ 0.0367, -0.1572,  0.0413,  0.3292]), 1.0, False, {})

Entering interaction loop! 

Random policy got 12.0 reward!
Starting new episode.
Random policy got 16.0 reward!
Starting new episode.
Random policy got 22.0 reward!
Starting new episode.
Random policy got 15.0 reward!
Starting new episode.
Random policy got 17.0 reward!
Starting new episode.

Interaction loop ended! Got reward 18.0 before episode was cut off.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="StateNormalizeWrapper" class="doc_header"><code>class</code> <code>StateNormalizeWrapper</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L73" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>StateNormalizeWrapper</code>(<strong><code>env</code></strong>:<code>Env</code>, <strong><code>beta</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.99</code></em>, <strong><code>eps</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>1e-08</code></em>) :: <code>Wrapper</code></p>
</blockquote>
<p>Environment wrapper for normalizing states.</p>
<p>Args:</p>
<ul>
<li>env (gym.Env): Environment to wrap.</li>
<li>beta (float): Beta parameter for running mean and variance calculation.</li>
<li>eps (float): Parameter to avoid division by zero in case variance goes to zero.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="StateNormalizeWrapper.reset" class="doc_header"><code>StateNormalizeWrapper.reset</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L108" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>StateNormalizeWrapper.reset</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Reset environment and return normalized state.</p>
<p>Returns:</p>
<ul>
<li>norm_state (np.array): Normalized state.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="StateNormalizeWrapper.normalize" class="doc_header"><code>StateNormalizeWrapper.normalize</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L93" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>StateNormalizeWrapper.normalize</code>(<strong><code>state</code></strong>:<code>array</code>)</p>
</blockquote>
<p>Update running mean and variance parameters and normalize input state.</p>
<p>Args:</p>
<ul>
<li>state (np.array): State to normalize and to use to calculate update.</li>
</ul>
<p>Returns:</p>
<ul>
<li>norm_state (np.array): Normalized state.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="StateNormalizeWrapper.step" class="doc_header"><code>StateNormalizeWrapper.step</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L119" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>StateNormalizeWrapper.step</code>(<strong><code>action</code></strong>:<code>Union</code>[<code>array</code>, <code>int</code>, <code>float</code>], <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Step environment and normalize state.</p>
<p>Args:</p>
<ul>
<li>action (np.array or int or float): Action to use to step the environment.</li>
</ul>
<p>Returns:</p>
<ul>
<li>norm_state (np.array): Normalized state.</li>
<li>reward (int or float): Reward earned at step.</li>
<li>done (bool): Whether the episode is over.</li>
<li>infos (dict): Any infos from the environment.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here is a demonstration of using the <a href="/rl_bolts/env_wrappers#StateNormalizeWrapper"><code>StateNormalizeWrapper</code></a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">StateNormalizeWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial obs:&quot;</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
<span class="c1"># the StateNormalizeWrapper expects NumPy arrays, so there is no need to convert action to PyTorch Tensor.</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">stepped</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;stepped once:&quot;</span><span class="p">,</span> <span class="n">stepped</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Entering interaction loop! </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># interaction loop</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random policy got </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> reward!&quot;</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">99</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting new episode.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interaction loop ended! Got reward </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> before episode was cut off.&quot;</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>initial obs: [-0.00917666  0.04326809 -0.0463752  -0.01014662]
stepped once: (array([-0.00826093,  0.23873963, -0.04634619, -0.31695305]), 1.0, False, {})

Entering interaction loop! 

Random policy got 46.0 reward!
Starting new episode.
Random policy got 12.0 reward!
Starting new episode.
Random policy got 23.0 reward!
Starting new episode.
Random policy got 16.0 reward!
Starting new episode.

Interaction loop ended! Got reward 3.0 before episode was cut off.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RewardScalerWrapper" class="doc_header"><code>class</code> <code>RewardScalerWrapper</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L137" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RewardScalerWrapper</code>(<strong><code>env</code></strong>:<code>Env</code>, <strong><code>beta</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.99</code></em>, <strong><code>eps</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>1e-08</code></em>) :: <code>Wrapper</code></p>
</blockquote>
<p>A class for reward scaling over training.</p>
<p>Calculates running mean and standard deviation of observed rewards and scales the rewards using the variance.</p>
<p>Computes: $r_t / (\sigma + eps)$</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RewardScalerWrapper.scale" class="doc_header"><code>RewardScalerWrapper.scale</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L154" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RewardScalerWrapper.scale</code>(<strong><code>reward</code></strong>:<code>Union</code>[<code>int</code>, <code>float</code>])</p>
</blockquote>
<p>Update running mean and variance for rewards, scale reward using the variance.</p>
<p>Args:</p>
<ul>
<li>reward (int or float): reward to scale.</li>
</ul>
<p>Returns:</p>
<ul>
<li>scaled_rew (float): reward scaled using variance.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RewardScalerWrapper.step" class="doc_header"><code>RewardScalerWrapper.step</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L171" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RewardScalerWrapper.step</code>(<strong><code>action</code></strong>, <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Step the environment and scale the reward.</p>
<p>Args:</p>
<ul>
<li>action (np.array or int or float): Action to use to step the environment.</li>
</ul>
<p>Returns:</p>
<ul>
<li>state (np.array): Next state from environment.</li>
<li>scaled_rew (float): reward scaled using the variance.</li>
<li>done (bool): Indicates whether the episode is over.</li>
<li>infos (dict): Any information from the environment.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>An example usage of the RewardScalerWrapper.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">RewardScalerWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial obs:&quot;</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">stepped</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;stepped once:&quot;</span><span class="p">,</span> <span class="n">stepped</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Entering interaction loop! </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># interaction loop</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random policy got </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> reward!&quot;</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">99</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting new episode.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interaction loop ended! Got reward </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> before episode was cut off.&quot;</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>initial obs: [-0.00811542 -0.00734643  0.00390471 -0.04174305]
stepped once: (array([-0.00826235,  0.18771931,  0.00306984, -0.33319146]), 1.0000995048508479, False, {})

Entering interaction loop! 

Random policy got 59.51370685735704 reward!
Starting new episode.

Interaction loop ended! Got reward 51.80559853548307 before episode was cut off.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Combining-Wrappers">Combining Wrappers<a class="anchor-link" href="#Combining-Wrappers"> </a></h2><p>All of these wrappers can be composed together! Simply be sure to call the <a href="/rl_bolts/env_wrappers#ToTorchWrapper"><code>ToTorchWrapper</code></a> last, because the others expect NumPy arrays as input, and the <a href="/rl_bolts/env_wrappers#ToTorchWrapper"><code>ToTorchWrapper</code></a> converts outputs to PyTorch tensors. Below is an example.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">StateNormalizeWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;After wrapping with StateNormalizeWrapper, output is still a NumPy array: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">RewardScalerWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;After wrapping with RewardScalerWrapper, output is still a NumPy array: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ToTorchWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;But after wrapping with ToTorchWrapper, output is now a PyTorch Tensor: </span><span class="si">{</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>After wrapping with StateNormalizeWrapper, output is still a NumPy array: [-0.04635718 -0.02190283 -0.01831842  0.02236239]
After wrapping with RewardScalerWrapper, output is still a NumPy array: [ 0.03441357 -0.02875284 -0.0059652   0.0248856 ]
But after wrapping with ToTorchWrapper, output is now a PyTorch Tensor: tensor([ 0.0004,  0.0037, -0.0323,  0.0158])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="BestPracticesWrapper" class="doc_header"><code>class</code> <code>BestPracticesWrapper</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L189" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>BestPracticesWrapper</code>(<strong><code>env</code></strong>:<code>Env</code>) :: <code>Wrapper</code></p>
</blockquote>
<p>This wrapper combines the wrappers which we think (from experience and from reading papers/blogs and watching lectures)
constitute best practices.</p>
<p>At the moment it combines the wrappers below in the order listed:</p>
<ol>
<li>StateNormalizeWrapper</li>
<li>RewardScalerWrapper</li>
<li>ToTorchWrapper</li>
</ol>
<p>Args:</p>
<ul>
<li>env (gym.Env): Environment to wrap.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="BestPracticesWrapper.reset" class="doc_header"><code>BestPracticesWrapper.reset</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L209" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>BestPracticesWrapper.reset</code>()</p>
</blockquote>
<p>Reset environment.</p>
<p>Returns:</p>
<ul>
<li>obs (torch.Tensor): Starting observations from the environment.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="BestPracticesWrapper.step" class="doc_header"><code>BestPracticesWrapper.step</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/env_wrappers.py#L219" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>BestPracticesWrapper.step</code>(<strong><code>action</code></strong>, <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>
<p>Step the environment forward using input action.</p>
<p>Args:</p>
<ul>
<li>action (torch.Tensor): Action to step the environment with.</li>
</ul>
<p>Returns:</p>
<ul>
<li>obs (torch.Tensor): Next step observations.</li>
<li>reward (int or float): Reward for the last timestep.</li>
<li>done (bool): Whether the episode is over.</li>
<li>infos (dict): Dictionary of any info from the environment.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below is a usage example of the <a href="/rl_bolts/env_wrappers#BestPracticesWrapper"><code>BestPracticesWrapper</code></a>. It is used in the same way as the <a href="/rl_bolts/env_wrappers#ToTorchWrapper"><code>ToTorchWrapper</code></a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">BestPracticesWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;initial obs:&quot;</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">stepped</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;stepped once:&quot;</span><span class="p">,</span> <span class="n">stepped</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Entering interaction loop! </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># interaction loop</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">ret</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Random policy got </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> reward!&quot;</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">99</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Starting new episode.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Interaction loop ended! Got reward </span><span class="si">{</span><span class="n">ret</span><span class="si">}</span><span class="s2"> before episode was cut off.&quot;</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>initial obs: tensor([ 0.0466, -0.0009, -0.0014, -0.0346])
stepped once: (tensor([ 0.0463, -0.1960, -0.0020,  0.2577]), 1.0000995048508479, False, {})

Entering interaction loop! 

Random policy got 44.18189402867389 reward!
Starting new episode.
Random policy got 48.143886812746324 reward!
Starting new episode.

Interaction loop ended! Got reward 18.993524551419902 before episode was cut off.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

