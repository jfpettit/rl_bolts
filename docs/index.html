---

title: rl_bolts 

keywords: fastai
sidebar: home_sidebar

summary: "rl_bolts is intended to be a package of nuts and bolts of RL algorithms, along with some full implementations of RL algorithms. "
description: "rl_bolts is intended to be a package of nuts and bolts of RL algorithms, along with some full implementations of RL algorithms. "
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>rl_bolts is starting as a package of just nuts and bolts of RL, and algorithms (and new nuts and bolts) will be added over time, based on necessity.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Install">Install<a class="anchor-link" href="#Install"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>git clone https://github.com/jfpettit/rl_bolts.git</code></p>
<p><code>cd rl_bolts</code></p>
<p><code>pip install -r requirements.txt</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Import the bits you need to use in your code.</p>
<p>The bit below sets up an actor-critic network for the CartPole-v1 gym environment.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">rl_bolts.neuralnets</span> <span class="k">as</span> <span class="nn">nns</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span>
<span class="n">actor_critic</span> <span class="o">=</span> <span class="n">nns</span><span class="o">.</span><span class="n">ActorCritic</span><span class="p">(</span>
    <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">env</span><span class="o">.</span><span class="n">action_space</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can print out the architecture of our actor_critic net below:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">actor_critic</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>ActorCritic(
  (policy): CategoricalPolicy(
    (net): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=4, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=32, bias=True)
        (2): Linear(in_features=32, out_features=2, bias=True)
      )
    )
  )
  (value_f): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=4, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
      (2): Linear(in_features=32, out_features=1, bias=True)
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">action</span><span class="p">,</span> <span class="n">logp</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The cell above starts the environment in a new episode, and passes it through the actor-critic to get an action, action log probability, and value estimate for the state.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;logp&quot;</span><span class="p">,</span> <span class="n">logp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>action tensor(0)
logp tensor(-0.6733)
value tensor(0.1155)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-a-pre-built-algorithm">Using a pre-built algorithm<a class="anchor-link" href="#Using-a-pre-built-algorithm"> </a></h2><p>While the primary aim of this package is to provide some building blocks for RL algorithms, we'll also provide implementations of a few plug-and-play algorithms. At present, we've implemented <a href="/rl_bolts/algorithms#PPO"><code>PPO</code></a> (it still needs to be thoroughly benchmarked, so be aware of that). Here is how to use it.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">rl_bolts.algorithms</span> <span class="kn">import</span> <span class="n">PPO</span> <span class="c1"># import the PPO algorithm</span>
<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span> <span class="c1"># PPO is a pytorch-lightning module, so need their library for Trainer.</span>
<span class="n">env_to_train_in</span> <span class="o">=</span> <span class="s2">&quot;CartPole-v1&quot;</span> <span class="c1"># set env to train PPO in. </span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="n">env_to_train_in</span><span class="p">)</span> <span class="c1"># initialize agent</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">reload_dataloaders_every_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># set up trainer, in practice you&#39;d set max_epochs to more than one</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">agent</span><span class="p">)</span> <span class="c1"># run trainer</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>GPU available: False, used: False
TPU available: False, using: 0 TPU cores

  | Name         | Type        | Params
---------------------------------------------
0 | actor_critic | ActorCritic | 2 K   
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch 1: 100%|██████████| 1/1 [00:00&lt;00:00,  1.44it/s, loss=96.740, v_num=5, PolicyLoss=6.82e-8, DeltaPolLoss=-.0247, KL=0.0125, Entropy=0.684, TimesEarlyStopped=0, AvgEarlyStopStep=0, ValueLoss=279, DeltaValLoss=-85.7]

MeanEpReturn: 22.587570621468927
StdEpReturn: 12.280362500254949
MaxEpReturn: 79.0
MinEpReturn: 9.0
MeanEpLength: 22.587570621468927
StdEpLength: 12.280362500254949
PolicyLoss: 6.818771680627833e-08
DeltaPolLoss: -0.024729875847697258
KL: 0.012456100434064865
Entropy: 0.6838463544845581
TimesEarlyStopped: 0
AvgEarlyStopStep: 0
ValueLoss: 279.19268798828125
DeltaValLoss: -85.71368408203125


Epoch 1: 100%|██████████| 1/1 [00:02&lt;00:00,  2.49s/it, loss=96.740, v_num=5, PolicyLoss=6.82e-8, DeltaPolLoss=-.0247, KL=0.0125, Entropy=0.684, TimesEarlyStopped=0, AvgEarlyStopStep=0, ValueLoss=279, DeltaValLoss=-85.7]
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

