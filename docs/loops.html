---

title: loops

keywords: fastai
sidebar: home_sidebar

summary: "This module will include some useful interaction loops for types of RL agents. It'll be updated over time."
description: "This module will include some useful interaction loops for types of RL agents. It'll be updated over time."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/06_loops.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="polgrad_interaction_loop" class="doc_header"><code>polgrad_interaction_loop</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/loops.py#L14" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>polgrad_interaction_loop</code>(<strong><code>env</code></strong>:<code>Env</code>, <strong><code>agent</code></strong>:<code>Module</code>, <strong><code>buffer</code></strong>:<a href="/rl_bolts/buffers#PGBuffer"><code>PGBuffer</code></a>, <strong><code>num_interactions</code></strong>:<code>int</code>=<em><code>4000</code></em>, <strong><code>horizon</code></strong>:<code>int</code>=<em><code>1000</code></em>)</p>
</blockquote>
<p>Interaction loop for actor-critic policy gradient agent.</p>
<p>This loop does not handle converting between PyTorch Tensors and NumPy arrays. So either your env should first be wrapped
in <a href="/rl_bolts/env_wrappers#ToTorchWrapper"><code>ToTorchWrapper</code></a> or your agent should accept and return NumPy arrays.</p>
<p>Args:</p>
<ul>
<li>env (gym.Env): Environment to run in.</li>
<li>agent (nn.Module): Agent to run within the environment, generates actions, values, and logprobs at each step.</li>
<li>buffer (rl_bolts.buffers.PGBuffer-like): Buffer object with same API and function signatures as the PGBuffer.</li>
<li>num_interactions (int): How many interactions to collect in the environment.</li>
<li>horizon (int): Maximum allowed episode length.</li>
</ul>
<p>Returns:</p>
<ul>
<li>buffer (rl_bolts.buffers.PGBuffer-like): Buffer filled with interactions.</li>
<li>infos (dict): Dictionary of reward and episode length statistics.</li>
<li>env_infos (list of dicts): List of all info dicts from the environment.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we demonstrate hypothetical usage of the interaction loop.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;CartPole-v1&quot;</span><span class="p">)</span> <span class="c1"># make the environment</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">env_wrappers</span><span class="o">.</span><span class="n">ToTorchWrapper</span><span class="p">(</span><span class="n">env</span><span class="p">)</span> <span class="c1"># wrap it for conversion to/from torch.Tensors</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">neuralnets</span><span class="o">.</span><span class="n">ActorCritic</span><span class="p">(</span> <span class="c1"># make the actor-critic agent</span>
    <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">buf</span> <span class="o">=</span> <span class="n">buffers</span><span class="o">.</span><span class="n">PGBuffer</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">4000</span><span class="p">)</span> <span class="c1"># create empty buffer</span>
<span class="n">full_buf</span><span class="p">,</span> <span class="n">infos</span><span class="p">,</span> <span class="n">env_infos</span> <span class="o">=</span> <span class="n">polgrad_interaction_loop</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">buf</span><span class="p">)</span> <span class="c1"># run loop, fills buffer</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">infos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> <span class="c1"># print loop stats</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>MeanEpReturn: 25.477707006369428
StdEpReturn: 14.071059873100223
MaxEpReturn: 100.0
MinEpReturn: 9.0
MeanEpLength: 25.477707006369428
StdEpLength: 14.071059873100223
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

