---

title: losses

keywords: fastai
sidebar: home_sidebar

summary: "This module defines losses for a variety of RL agents."
description: "This module defines losses for a variety of RL agents."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/04_losses.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="actor_critic_value_loss" class="doc_header"><code>actor_critic_value_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L15" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>actor_critic_value_loss</code>(<strong><code>value_estimates</code></strong>:<code>Tensor</code>, <strong><code>env_returns</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Loss for an actor-critic value function.</p>
<p>Is just Mean-Squared-Error between the value estimates and the real returns.</p>
<p>Args:</p>
<ul>
<li>value_estimates (torch.Tensor): Estimates of state-value from the critic network.</li>
<li>env_returns (torch.Tensor): Real returns from the environment.</li>
</ul>
<p>Returns:</p>
<ul>
<li>value_loss (torch.Tensor): MSE loss betwen the estimates and real returns.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="reinforce_policy_loss" class="doc_header"><code>reinforce_policy_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L33" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>reinforce_policy_loss</code>(<strong><code>logps</code></strong>:<code>Tensor</code>, <strong><code>env_returns</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Reinforce Policy gradient loss. $-(log(\pi(a | s)) * R_t)$</p>
<p>Args:</p>
<ul>
<li>logps (PyTorch Tensor): Action log probabilities.</li>
<li>env_returns (PyTorch Tensor): Returns from the environment.</li>
</ul>
<p>Returns:</p>
<ul>
<li>reinforce_loss (torch.Tensor): REINFORCE loss term.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="a2c_policy_loss" class="doc_header"><code>a2c_policy_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L49" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>a2c_policy_loss</code>(<strong><code>logps</code></strong>:<code>Tensor</code>, <strong><code>advs</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Loss function for an A2C policy. $-(logp(\pi(a|s)) * A_t)$</p>
<p>Args:</p>
<ul>
<li>logps (torch.Tensor): Log-probabilities of selected actions.</li>
<li>advs (torch.Tensor): Advantage estimates of selected actions.</li>
</ul>
<p>Returns:</p>
<ul>
<li>a2c_loss (torch.Tensor): A2C loss term.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ppo_clip_policy_loss" class="doc_header"><code>ppo_clip_policy_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L64" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ppo_clip_policy_loss</code>(<strong><code>logps</code></strong>:<code>Tensor</code>, <strong><code>logps_old</code></strong>:<code>Tensor</code>, <strong><code>advs</code></strong>:<code>Tensor</code>, <strong><code>clipratio</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.2</code></em>)</p>
</blockquote>
<p>Loss function for a PPO-clip policy.
See paper for full loss function math: <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a></p>
<p>Args:</p>
<ul>
<li>logps (torch.Tensor): Action log-probabilities under the current policy.</li>
<li>logps_old (torch.Tensor): Action log-probabilities under the old (pre-update) policy.</li>
<li>advs (torch.Tensor): Advantage estimates for the actions taken.</li>
<li>clipratio (float): Clipping parameter for PPO-clip loss. In general, is fine with being left as default.</li>
</ul>
<p>Returns:</p>
<ul>
<li>ppo_loss (torch.Tensor): Loss term for PPO agent.</li>
<li>kl (torch.Tensor): KL-divergence estimate between new and old policies.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ddpg_policy_loss" class="doc_header"><code>ddpg_policy_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L92" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ddpg_policy_loss</code>(<strong><code>states</code></strong>:<code>Tensor</code>, <strong><code>qfunc</code></strong>:<code>Module</code>, <strong><code>policy</code></strong>:<code>Module</code>)</p>
</blockquote>
<p>Policy loss function for DDPG agent. See the paper: <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></p>
<p>Args:</p>
<ul>
<li>states (torch.Tensor): States to get Q-policy estimates for.</li>
<li>qfunc (nn.Module): Q-function network.</li>
<li>policy (nn.Module): Policy network.</li>
</ul>
<p>Returns:</p>
<ul>
<li>q_policy_loss (torch.Tensor): Loss term for DDPG policy.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ddpg_qfunc_loss" class="doc_header"><code>ddpg_qfunc_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L110" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ddpg_qfunc_loss</code>(<strong><code>data</code></strong>:<code>Tuple</code>[<code>Tensor</code>, <code>Tensor</code>, <code>Tensor</code>, <code>Tensor</code>, <code>Tensor</code>], <strong><code>qfunc</code></strong>:<code>Module</code>, <strong><code>qfunc_target</code></strong>:<code>Module</code>, <strong><code>policy_target</code></strong>:<code>Module</code>, <strong><code>gamma</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.99</code></em>)</p>
</blockquote>
<p>Loss for a DDPG Q-function. See the paper: <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></p>
<p>Args:</p>
<ul>
<li>data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the
following: (states, next_states, actions, rewards, dones).</li>
<li>qfunc (nn.Module): Q-function network being trained.</li>
<li>qfunc_target (nn.Module): Q-function target network.</li>
<li>policy_target (nn.Module): Policy target network.</li>
<li>gamma (float): Discount factor.</li>
</ul>
<p>Returns:</p>
<ul>
<li>loss_q (torch.Tensor): DDPG loss for the Q-function.</li>
<li>loss_info (dict): Dictionary containing useful loss info for logging.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="td3_policy_loss" class="doc_header"><code>td3_policy_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L150" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>td3_policy_loss</code>(<strong><code>states</code></strong>:<code>Tensor</code>, <strong><code>qfunc</code></strong>:<code>Module</code>, <strong><code>policy</code></strong>:<code>Module</code>)</p>
</blockquote>
<p>Calculate policy loss for TD3 agent. See paper here: <a href="https://arxiv.org/abs/1802.09477">https://arxiv.org/abs/1802.09477</a></p>
<p>Args:</p>
<ul>
<li>states (torch.Tensor): Input states to get policy loss for.</li>
<li>qfunc (torch.Tensor): TD3 q-function network.</li>
<li>policy (torch.Tensor): Policy network.</li>
</ul>
<p>Returns:</p>
<ul>
<li>q_policy_loss (torch.Tensor): The TD3 policy loss term.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="td3_qfunc_loss" class="doc_header"><code>td3_qfunc_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L167" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>td3_qfunc_loss</code>(<strong><code>data</code></strong>:<code>Tuple</code>[<code>Tensor</code>, <code>Tensor</code>, <code>Tensor</code>, <code>Tensor</code>, <code>Tensor</code>], <strong><code>qfunc1</code></strong>:<code>Module</code>, <strong><code>qfunc2</code></strong>:<code>Module</code>, <strong><code>qfunc1_target</code></strong>:<code>Module</code>, <strong><code>qfunc2_target</code></strong>:<code>Module</code>, <strong><code>policy</code></strong>:<code>Module</code>, <strong><code>act_limit</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>], <strong><code>target_noise</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.2</code></em>, <strong><code>noise_clip</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.5</code></em>, <strong><code>gamma</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.99</code></em>)</p>
</blockquote>
<p>Calculate Q-function loss for TD3 agent. See paper here: <a href="https://arxiv.org/abs/1802.09477">https://arxiv.org/abs/1802.09477</a></p>
<p>Args:</p>
<ul>
<li>data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the
following: (states, next_states, actions, rewards, dones).</li>
<li>qfunc1 (nn.Module): First Q-function network being trained.</li>
<li>qfunc2 (nn.Module): Other Q-function network being trained.</li>
<li>qfunc1_target (nn.Module): First Q-function target network.</li>
<li>qfunc2_target (nn.Module): Other Q-function target network.</li>
<li>policy (nn.Module): Policy network.</li>
<li>act_limit (float or int): Action limit from the environment.</li>
<li>target_noise (float): Noise to apply to policy target network.</li>
<li>noise_clip (float): Clip the noise within + and - this range.</li>
<li>gamma (float): Gamma discount factor.</li>
</ul>
<p>Returns:</p>
<ul>
<li>loss_q (torch.Tensor): TD3 loss for the Q-function.</li>
<li>loss_info (dict): Dictionary containing useful loss info for logging.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="sac_policy_loss" class="doc_header"><code>sac_policy_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L231" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>sac_policy_loss</code>(<strong><code>states</code></strong>:<code>Tensor</code>, <strong><code>qfunc1</code></strong>:<code>Module</code>, <strong><code>qfunc2</code></strong>:<code>Module</code>, <strong><code>policy</code></strong>:<code>Module</code>, <strong><code>alpha</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.2</code></em>)</p>
</blockquote>
<p>Calculate policy loss for Soft-Actor Critic agent. See paper here: <a href="https://arxiv.org/abs/1801.01290">https://arxiv.org/abs/1801.01290</a></p>
<p>Args:</p>
<ul>
<li>states (torch.Tensor): Input states for the policy.</li>
<li>qfunc1 (nn.Module): First Q-function in SAC agent.</li>
<li>qfunc2 (nn.Module): Second Q-function in SAC agent.</li>
<li>policy (nn.Module): Policy network.</li>
<li>alpha (float): alpha factor for entropy-regularized policy loss.</li>
</ul>
<p>Returns:</p>
<ul>
<li>loss_policy (torch.Tensor): The policy loss term.</li>
<li>policy_info (dict): Useful logging info for the policy.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="sac_qfunc_loss" class="doc_header"><code>sac_qfunc_loss</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/losses.py#L267" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>sac_qfunc_loss</code>(<strong><code>data</code></strong>, <strong><code>qfunc1</code></strong>:<code>Module</code>, <strong><code>qfunc2</code></strong>:<code>Module</code>, <strong><code>qfunc1_target</code></strong>:<code>Module</code>, <strong><code>qfunc2_target</code></strong>:<code>Module</code>, <strong><code>policy</code></strong>:<code>Module</code>, <strong><code>gamma</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.99</code></em>, <strong><code>alpha</code></strong>:<code>Optional</code>[<code>float</code>]=<em><code>0.2</code></em>)</p>
</blockquote>
<p>Q-function loss for Soft-Actor Critic agent.</p>
<p>Args:</p>
<ul>
<li>data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the
following: (states, next_states, actions, rewards, dones).</li>
<li>qfunc1 (nn.Module): First Q-function network being trained.</li>
<li>qfunc2 (nn.Module): Other Q-function network being trained.</li>
<li>qfunc1_target (nn.Module): First Q-function target network.</li>
<li>qfunc2_target (nn.Module): Other Q-function target network.</li>
<li>policy (nn.Module): Policy network.</li>
<li>gamma (float): Gamma discount factor.</li>
<li>alpha (float): Loss term alpha factor.</li>
</ul>
<p>Returns:</p>
<ul>
<li>loss_q (torch.Tensor): SAC loss for the Q-function.</li>
<li>loss_info (dict): Dictionary containing useful loss info for logging.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

