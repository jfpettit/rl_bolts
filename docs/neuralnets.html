---

title: neuralnets

keywords: fastai
sidebar: home_sidebar

summary: "Implementations of RL policies, value functions, and actor-critic networks."
description: "Implementations of RL policies, value functions, and actor-critic networks."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/03_neuralnets.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MLP" class="doc_header"><code>class</code> <code>MLP</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L18" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MLP</code>(<strong><code>layer_sizes</code></strong>:<code>Union</code>[<code>List</code>[<code>T</code>], <code>Tuple</code>], <strong><code>activations</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>'tanh'</code></em>, <strong><code>out_act</code></strong>:<code>Optional</code>[<code>bool</code>]=<em><code>None</code></em>, <strong><code>out_squeeze</code></strong>:<code>Optional</code>[<code>bool</code>]=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>A class for building a simple MLP network.</p>
<p>Args:</p>
<ul>
<li>layer_sizes (list or tuple): Layer sizes for the network.</li>
<li>activations (Function): Activation function for MLP net.</li>
<li>out_act (Function): Output activation function</li>
<li>out_squeeze (bool): Whether to squeeze the output of the network.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Actor" class="doc_header"><code>class</code> <code>Actor</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L58" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Actor</code>() :: <code>Module</code></p>
</blockquote>
<p>Barebones class structure for an Actor.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Actor.forward" class="doc_header"><code>Actor.forward</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L68" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Actor.forward</code>(<strong><code>x</code></strong>, <strong><code>a</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>Forward pass for an policy.</p>
<p>Args:</p>
<ul>
<li>x (torch.Tensor): Input state from the environment.</li>
<li>a (torch.Tensor): Action that was taken.</li>
</ul>
<p>Returns:</p>
<ul>
<li>policy (PyTorch distribution): The policy distribution.</li>
<li>logp_a (torch.Tensor): Log-probability of input action under the policy distribution.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CategoricalPolicy" class="doc_header"><code>class</code> <code>CategoricalPolicy</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L87" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CategoricalPolicy</code>(<strong><code>state_features</code></strong>:<code>int</code>, <strong><code>action_dim</code></strong>:<code>int</code>, <strong><code>hidden_sizes</code></strong>:<code>Union</code>[<code>List</code>[<code>T</code>], <code>Tuple</code>], <strong><code>activation</code></strong>:<code>Callable</code>, <strong><code>out_activation</code></strong>:<code>Callable</code>) :: <a href="/rl_bolts/neuralnets#Actor"><code>Actor</code></a></p>
</blockquote>
<p>A class for a Categorical Policy network. Used in discrete action space environments.</p>
<p>The policy is an <a href="/rl_bolts/neuralnets#MLP"><code>MLP</code></a>.</p>
<p>Args:</p>
<ul>
<li>state_features (int): Dimensionality of the state space.</li>
<li>action_dim (int): Dimensionality of the action space.</li>
<li>hidden_sizes (list or tuple): Hidden layer sizes.</li>
<li>activation (Function): Activation function for the network.</li>
<li>out_activation (Function): Output activation function for the network.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="CategoricalPolicy.logprob_from_distribution" class="doc_header"><code>CategoricalPolicy.logprob_from_distribution</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L127" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>CategoricalPolicy.logprob_from_distribution</code>(<strong><code>policy</code></strong>:<code>Distribution</code>, <strong><code>actions</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Calculate the log-probability of an action under a policy.</p>
<p>Args:</p>
<ul>
<li>policy (torch.distributions.Distribution): The policy distribution over input state.</li>
<li>actions (torch.Tensor): Actions to take log probability of.</li>
</ul>
<p>Returns:</p>
<ul>
<li>log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="CategoricalPolicy.action_distribution" class="doc_header"><code>CategoricalPolicy.action_distribution</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L114" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>CategoricalPolicy.action_distribution</code>(<strong><code>x</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Defines action distribution conditioned on input state.</p>
<p>Args:</p>
<ul>
<li>x(torch.Tensor): input state</li>
</ul>
<p>Returns:</p>
<ul>
<li>Categorical distribution: Policy over the action space.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GaussianPolicy" class="doc_header"><code>class</code> <code>GaussianPolicy</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L141" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GaussianPolicy</code>(<strong><code>state_features</code></strong>:<code>int</code>, <strong><code>action_dim</code></strong>:<code>int</code>, <strong><code>hidden_sizes</code></strong>:<code>Union</code>[<code>List</code>[<code>T</code>], <code>Tuple</code>], <strong><code>activation</code></strong>:<code>Callable</code>, <strong><code>out_activation</code></strong>:<code>Callable</code>) :: <a href="/rl_bolts/neuralnets#Actor"><code>Actor</code></a></p>
</blockquote>
<p>A class for a Gaussian Policy network. Used in continuous action space environments. The policy is an <a href="/rl_bolts/neuralnets#MLP"><code>MLP</code></a>.</p>
<p>Args:</p>
<ul>
<li>state_features (int): Dimensionality of the state space.</li>
<li>action_dim (int): Dimensionality of the action space.</li>
<li>hidden_sizes (list or tuple): Hidden layer sizes.</li>
<li>activation (Function): Activation function for the network.</li>
<li>out_activation (Function): Output activation function for the network.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaussianPolicy.action_distribution" class="doc_header"><code>GaussianPolicy.action_distribution</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L171" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaussianPolicy.action_distribution</code>(<strong><code>states</code></strong>)</p>
</blockquote>
<p>Defines action distribution conditioned on input state.</p>
<p>Args:</p>
<ul>
<li>x(torch.Tensor): input state</li>
</ul>
<p>Returns:</p>
<ul>
<li>Normal distribution: Policy over the action space.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="GaussianPolicy.logprob_from_distribution" class="doc_header"><code>GaussianPolicy.logprob_from_distribution</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L185" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>GaussianPolicy.logprob_from_distribution</code>(<strong><code>policy</code></strong>, <strong><code>actions</code></strong>)</p>
</blockquote>
<p>Calculate the log-probability of an action under a policy.</p>
<p>Args:</p>
<ul>
<li>policy (torch.distributions.Distribution): The policy distribution over input state.</li>
<li>actions (torch.Tensor): Actions to take log probability of.</li>
</ul>
<p>Returns:</p>
<ul>
<li>log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ActorCritic" class="doc_header"><code>class</code> <code>ActorCritic</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L199" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ActorCritic</code>(<strong><code>state_features</code></strong>:<code>int</code>, <strong><code>action_space</code></strong>:<code>int</code>, <strong><code>hidden_sizes</code></strong>:<code>Union</code>[<code>Tuple</code>, <code>List</code>[<code>T</code>], <code>NoneType</code>]=<em><code>(32, 32)</code></em>, <strong><code>activation</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>'tanh'</code></em>, <strong><code>out_activation</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>policy</code></strong>:<code>Optional</code>[<code>Module</code>]=<em><code>None</code></em>) :: <code>Module</code></p>
</blockquote>
<p>An Actor Critic class for Policy Gradient algorithms.</p>
<p>Has built-in capability to work with continuous (gym.spaces.Box) and discrete (gym.spaces.Discrete) action spaces.
The policy and value function are both <a href="/rl_bolts/neuralnets#MLP"><code>MLP</code></a>.</p>
<p>If working with a different action space,
the user can pass in a custom policy class for that action space as an argument.</p>
<p>Args:</p>
<ul>
<li>state_features (int): Dimensionality of the state space.</li>
<li>action_space (gym.spaces.Space): Action space of the environment.</li>
<li>hidden_sizes (list or tuple): Hidden layer sizes.</li>
<li>activation (Function): Activation function for the network.</li>
<li>out_activation (Function): Output activation function for the network.</li>
<li>policy (nn.Module): Custom policy class for an environment where the action space is not gym.spaces.Box or gym.spaces.Discrete</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ActorCritic.step" class="doc_header"><code>ActorCritic.step</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L257" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ActorCritic.step</code>(<strong><code>x</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Get action, action log probability, and value estimate for an input state.</p>
<p>Args:</p>
<ul>
<li>x (torch.Tensor): input state.</li>
</ul>
<p>Returns:</p>
<ul>
<li>action (torch.Tensor): Action chosen by the policy.</li>
<li>logp_action (torch.Tensor): Log probability of that action chosen by the policy.</li>
<li>value (torch.Tensor): Value estimate of the current state.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ActorCritic.act" class="doc_header"><code>ActorCritic.act</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L276" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ActorCritic.act</code>(<strong><code>x</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Similar to <code>step</code>, but get only the action.</p>
<p>Args:</p>
<ul>
<li>x (torch.Tensor): input state</li>
</ul>
<p>Returns:</p>
<ul>
<li>action (torch.Tensor): Action chosen by the policy.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MLPQActor" class="doc_header"><code>class</code> <code>MLPQActor</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L290" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MLPQActor</code>(<strong><code>state_features</code></strong>:<code>int</code>, <strong><code>action_dim</code></strong>:<code>int</code>, <strong><code>hidden_sizes</code></strong>:<code>Union</code>[<code>list</code>, <code>tuple</code>], <strong><code>activation</code></strong>:<code>Callable</code>, <strong><code>action_limit</code></strong>:<code>Union</code>[<code>float</code>, <code>int</code>]) :: <code>Module</code></p>
</blockquote>
<p>An actor for Q policy gradient algorithms.</p>
<p>The policy is an <a href="/rl_bolts/neuralnets#MLP"><code>MLP</code></a>.
The output from the policy network is scaled to action space limits on the forward pass.</p>
<p>Args:</p>
<ul>
<li>state_features (int): Dimensionality of the state space.</li>
<li>action_dim (int): Dimensionality of the action space.</li>
<li>hidden_sizes (list or tuple): Hidden layer sizes.</li>
<li>activation (Function): Activation function for the network.</li>
<li>action_limit (float or int): Limits of the action space.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MLPQActor.forward" class="doc_header"><code>MLPQActor.forward</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L318" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MLPQActor.forward</code>(<strong><code>x</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Return output from the policy network scaled to the limits of the env action space.
Args:</p>
<ul>
<li>x (torch.Tensor): States from environment.</li>
</ul>
<p>Returns:</p>
<ul>
<li>scaled_action (torch.Tensor): Action scaled to action space limits.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MLPQFunction" class="doc_header"><code>class</code> <code>MLPQFunction</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L331" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MLPQFunction</code>(<strong><code>state_features</code></strong>:<code>int</code>, <strong><code>action_dim</code></strong>:<code>int</code>, <strong><code>hidden_sizes</code></strong>:<code>Union</code>[<code>tuple</code>, <code>list</code>], <strong><code>activation</code></strong>:<code>Callable</code>) :: <code>Module</code></p>
</blockquote>
<p>A Q function network for Q policy gradient methods.</p>
<p>The Q function is an <a href="/rl_bolts/neuralnets#MLP"><code>MLP</code></a>. It always takes in a (state, action) pair and returns a Q-value estimate for that pair.</p>
<p>Args:</p>
<ul>
<li>state_features (int): Dimensionality of the state space.</li>
<li>action_dim (int): Dimensionality of the action space.</li>
<li>hidden_sizes (list or tuple): Hidden layer sizes.</li>
<li>activation (Function): Activation function for the network.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="MLPQFunction.forward" class="doc_header"><code>MLPQFunction.forward</code><a href="https://github.com/jfpettit/rl_bolts/tree/master/rl_bolts/neuralnets.py#L356" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>MLPQFunction.forward</code>(<strong><code>x</code></strong>:<code>Tensor</code>, <strong><code>a</code></strong>:<code>Tensor</code>)</p>
</blockquote>
<p>Return Q-value estimate for state, action pair (x, a).</p>
<p>Args:</p>
<ul>
<li>x (torch.Tensor): Environment state.</li>
<li>a (torch.Tensor): Action taken by the policy.</li>
</ul>
<p>Returns:</p>
<ul>
<li>q (torch.Tensor): Q-value estimate for state action pair.</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

