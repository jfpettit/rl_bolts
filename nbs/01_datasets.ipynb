{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets\n",
    "\n",
    "> Here we define some useful dataset classes for converting data from buffers of NumPy arrays to PyTorch Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class PolicyGradientRLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for policy gradient RL algorithms.\n",
    "\n",
    "    It returns a tuple of (state, action, advantage, reward, action_logp) at each index.\n",
    "\n",
    "    Args:\n",
    "    - data (NumPy array): Batch of interaction data to train on.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data\n",
    "    ):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[2])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        state = self.data[0][idx]\n",
    "        act = self.data[1][idx]\n",
    "        adv = self.data[2][idx]\n",
    "        rew = self.data[3][idx]\n",
    "        logp = self.data[4][idx]\n",
    "\n",
    "        return state, act, adv, rew, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class QPolicyGradientRLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset for Q policy gradient algorithms.\n",
    "\n",
    "    It returns a tuple of (state, next_state, action, reward, done).\n",
    "\n",
    "    Args:\n",
    "    - data (NumPy array): Numpy array of data to train on.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data\n",
    "    ):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[3])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obs = self.data[0][idx]\n",
    "        obs2 = self.data[1][idx]\n",
    "        act = self.data[2][idx]\n",
    "        rew = self.data[3][idx]\n",
    "        done = self.data[4][idx]\n",
    "        return obs, obs2, act, rew, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
