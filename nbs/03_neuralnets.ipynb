{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp neuralnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralnets\n",
    "\n",
    "> Implementations of RL policies, value functions, and actor-critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import signal\n",
    "import gym\n",
    "from scipy.signal import lfilter\n",
    "from typing import Optional, Iterable, List, Dict, Callable, Union, Tuple\n",
    "from rl_bolts.env_wrappers import ToTorchWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class MLP(nn.Module):\n",
    "    r\"\"\"\n",
    "    A class for building a simple MLP network.\n",
    "\n",
    "    Args:\n",
    "    - layer_sizes (list or tuple): Layer sizes for the network.\n",
    "    - activations (Function): Activation function for MLP net.\n",
    "    - out_act (Function): Output activation function\n",
    "    - out_squeeze (bool): Whether to squeeze the output of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: Union[List, Tuple],\n",
    "        activations: Optional[Callable] = torch.tanh,\n",
    "        out_act: Optional[bool] = None,\n",
    "        out_squeeze: Optional[bool] = False,\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = activations\n",
    "        self.out_act = out_act\n",
    "        self.out_squeeze = out_squeeze\n",
    "\n",
    "        for i, l in enumerate(layer_sizes[1:]):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], l))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for MLP network\"\"\"\n",
    "        for l in self.layers[:-1]:\n",
    "            x = self.activations(l(x))\n",
    "\n",
    "        if self.out_act is None:\n",
    "            x = self.layers[-1](x)\n",
    "        else:\n",
    "            x = self.out_act(self.layers[-1](x))\n",
    "\n",
    "        return torch.squeeze(x, -1) if self.out_squeeze else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "mlp = MLP((3, 5, 7, 1))\n",
    "inp = torch.randn(3)\n",
    "assert mlp(inp) is not None, \"MLP is failing to forward propagate.\"\n",
    "assert mlp(inp).shape[0] == 1, \"MLP is mapping to wrong dimension.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"MLP\" class=\"doc_header\"><code>class</code> <code>MLP</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>MLP</code>(**`layer_sizes`**:`Union`\\[`List`\\[`T`\\], `Tuple`\\], **`activations`**:`Optional`\\[`Callable`\\]=*`'tanh'`*, **`out_act`**:`Optional`\\[`bool`\\]=*`None`*, **`out_squeeze`**:`Optional`\\[`bool`\\]=*`False`*) :: `Module`\n",
       "\n",
       "A class for building a simple MLP network.\n",
       "\n",
       "Args:\n",
       "- layer_sizes (list or tuple): Layer sizes for the network.\n",
       "- activations (Function): Activation function for MLP net.\n",
       "- out_act (Function): Output activation function\n",
       "- out_squeeze (bool): Whether to squeeze the output of the network."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Create a PyTorch CNN module.\n",
    "    \n",
    "    Args:\n",
    "    - input_channels (int): number of channels in the input\n",
    "    - input_height (int): size of one side of input (currently assumes square input)\n",
    "    - output_size (int): size of network output\n",
    "    - kernel_size (int): Convolutional kernel size\n",
    "    - stride (int): convolutional kernel stride\n",
    "    - channels (list or tuple): List of channel sizes for each convolutional layer\n",
    "    - linear_layer_sizes (list or tuple): list of (if any) sizes of linear layers to add after convolutional layers\n",
    "    - activation (callable): activation function\n",
    "    - output_activation (int): if any, activation to apply to the output layer\n",
    "    - dropout_layers (list or tuple): if any, layers to apply dropout to\n",
    "    - dropout_p (float): probability of dropout to use\n",
    "    - out_squeeze (bool): whether to squeeze the output\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "            input_channels: int,\n",
    "            input_height: int,\n",
    "            output_size: int,\n",
    "            kernel_size: Optional[int] = 3,\n",
    "            stride: Optional[int] = 1,\n",
    "            channels: Optional[list] = [64, 64],\n",
    "            linear_layer_sizes: Optional[list] = [512],\n",
    "            activation: Optional[Callable] = torch.relu,\n",
    "            output_activation: Optional[Callable] = None,\n",
    "            dropout_layers: Optional[list] = None,\n",
    "            dropout_p: Optional[float] = None,\n",
    "            out_squeeze: Optional[bool] = False\n",
    "        ):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        conv_sizes = [input_channels] + channels\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.out_squeeze = out_squeeze\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout_layers = dropout_layers\n",
    "\n",
    "        self.hw=input_height\n",
    "        for i, l in enumerate(conv_sizes[1:]):\n",
    "            self.hw = conv2d_output_size(kernel_size=kernel_size, stride=stride, sidesize=self.hw)\n",
    "            self.layers.append(nn.Conv2d(conv_sizes[i], l, kernel_size=kernel_size, stride=stride))\n",
    "\n",
    "        self.hw = (self.hw, self.hw)\n",
    "        conv_out_size = 1\n",
    "        for num in self.hw:\n",
    "            conv_out_size *= num\n",
    "        conv_out_size *= conv_sizes[-1]\n",
    "        linear_sizes = [conv_out_size] + linear_layer_sizes + [output_size]\n",
    "        self.layers.append(nn.Flatten())\n",
    "        for i, l in enumerate(linear_sizes[1:]):\n",
    "            self.layers.append(nn.Linear(linear_sizes[i], l))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for l in self.layers[:-1]:\n",
    "            x = self.activation(l(x))\n",
    "\n",
    "            if self.dropout_layers is not None and l in self.dropout_layers:\n",
    "                x = F.dropout(x, p=self.dropout_p)\n",
    "\n",
    "        if self.output_activation is None:\n",
    "            x = self.layers[-1](x)\n",
    "        else:\n",
    "            x = self.output_activation(self.layers[-1](x))\n",
    "\n",
    "        return x.squeeze() if self.out_squeeze else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CNN\" class=\"doc_header\"><code>class</code> <code>CNN</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CNN</code>(**`input_channels`**:`int`, **`input_height`**:`int`, **`output_size`**:`int`, **`kernel_size`**:`Optional`\\[`int`\\]=*`3`*, **`stride`**:`Optional`\\[`int`\\]=*`1`*, **`channels`**:`Optional`\\[`list`\\]=*`[64, 64]`*, **`linear_layer_sizes`**:`Optional`\\[`list`\\]=*`[512]`*, **`activation`**:`Optional`\\[`Callable`\\]=*`'relu'`*, **`output_activation`**:`Optional`\\[`Callable`\\]=*`None`*, **`dropout_layers`**:`Optional`\\[`list`\\]=*`None`*, **`dropout_p`**:`Optional`\\[`float`\\]=*`None`*, **`out_squeeze`**:`Optional`\\[`bool`\\]=*`False`*) :: `Module`\n",
       "\n",
       "Create a PyTorch CNN module.\n",
       "\n",
       "Args:\n",
       "- input_channels (int): number of channels in the input\n",
       "- input_height (int): size of one side of input (currently assumes square input)\n",
       "- output_size (int): size of network output\n",
       "- kernel_size (int): Convolutional kernel size\n",
       "- stride (int): convolutional kernel stride\n",
       "- channels (list or tuple): List of channel sizes for each convolutional layer\n",
       "- linear_layer_sizes (list or tuple): list of (if any) sizes of linear layers to add after convolutional layers\n",
       "- activation (callable): activation function\n",
       "- output_activation (int): if any, activation to apply to the output layer\n",
       "- dropout_layers (list or tuple): if any, layers to apply dropout to\n",
       "- dropout_p (float): probability of dropout to use\n",
       "- out_squeeze (bool): whether to squeeze the output"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    Barebones class structure for an Actor.\n",
    "    \"\"\"\n",
    "    def action_distribution(self, states):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def logprob_from_distribution(self, policy, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, a = None):\n",
    "        \"\"\"\n",
    "        Forward pass for an policy.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input state from the environment.\n",
    "        - a (torch.Tensor): Action that was taken.\n",
    "\n",
    "        Returns:\n",
    "        - policy (PyTorch distribution): The policy distribution.\n",
    "        - logp_a (torch.Tensor): Log-probability of input action under the policy distribution.\n",
    "        \"\"\"\n",
    "        policy = self.action_distribution(x)\n",
    "        logp_a = None\n",
    "        if a is not None:\n",
    "            logp_a = self.logprob_from_distribution(policy, a)\n",
    "        return policy, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Actor\" class=\"doc_header\"><code>class</code> <code>Actor</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Actor</code>() :: `Module`\n",
       "\n",
       "Barebones class structure for an Actor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Actor.forward\" class=\"doc_header\"><code>Actor.forward</code><a href=\"__main__.py#L12\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Actor.forward</code>(**`x`**, **`a`**=*`None`*)\n",
       "\n",
       "Forward pass for an policy.\n",
       "\n",
       "Args:\n",
       "- x (torch.Tensor): Input state from the environment.\n",
       "- a (torch.Tensor): Action that was taken.\n",
       "\n",
       "Returns:\n",
       "- policy (PyTorch distribution): The policy distribution.\n",
       "- logp_a (torch.Tensor): Log-probability of input action under the policy distribution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Actor.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class CategoricalPolicy(Actor):\n",
    "    r\"\"\"\n",
    "    A class for a Categorical Policy network. Used in discrete action space environments.\n",
    "\n",
    "    The policy is an `MLP`.\n",
    "\n",
    "    Args:\n",
    "    - state_features (int): Dimensionality of the state space.\n",
    "    - action_dim (int): Dimensionality of the action space.\n",
    "    - hidden_sizes (list or tuple): Hidden layer sizes.\n",
    "    - activation (Function): Activation function for the network.\n",
    "    - out_activation (Function): Output activation function for the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_features: int,\n",
    "        action_dim: int,\n",
    "        hidden_sizes: Union[List, Tuple],\n",
    "        activation: Callable,\n",
    "        out_activation: Callable,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = MLP(\n",
    "            [state_features] + list(hidden_sizes) + [action_dim], activations=activation\n",
    "        )\n",
    "\n",
    "    def action_distribution(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Defines action distribution conditioned on input state.\n",
    "\n",
    "        Args:\n",
    "        - x(torch.Tensor): input state\n",
    "\n",
    "        Returns:\n",
    "        - Categorical distribution: Policy over the action space.\n",
    "        \"\"\"\n",
    "        logits = self.net(x)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    def logprob_from_distribution(self, policy: torch.distributions.Distribution, actions: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Calculate the log-probability of an action under a policy.\n",
    "\n",
    "        Args:\n",
    "        - policy (torch.distributions.Distribution): The policy distribution over input state.\n",
    "        - actions (torch.Tensor): Actions to take log probability of.\n",
    "\n",
    "        Returns:\n",
    "        - log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution.\n",
    "        \"\"\"\n",
    "        return policy.log_prob(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CategoricalPolicy\" class=\"doc_header\"><code>class</code> <code>CategoricalPolicy</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CategoricalPolicy</code>(**`state_features`**:`int`, **`action_dim`**:`int`, **`hidden_sizes`**:`Union`\\[`List`\\[`T`\\], `Tuple`\\], **`activation`**:`Callable`, **`out_activation`**:`Callable`) :: [`Actor`](/rl_bolts/neuralnets#Actor)\n",
       "\n",
       "A class for a Categorical Policy network. Used in discrete action space environments.\n",
       "\n",
       "The policy is an [`MLP`](/rl_bolts/neuralnets#MLP).\n",
       "\n",
       "Args:\n",
       "- state_features (int): Dimensionality of the state space.\n",
       "- action_dim (int): Dimensionality of the action space.\n",
       "- hidden_sizes (list or tuple): Hidden layer sizes.\n",
       "- activation (Function): Activation function for the network.\n",
       "- out_activation (Function): Output activation function for the network."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CategoricalPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CategoricalPolicy.logprob_from_distribution\" class=\"doc_header\"><code>CategoricalPolicy.logprob_from_distribution</code><a href=\"__main__.py#L42\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CategoricalPolicy.logprob_from_distribution</code>(**`policy`**:`Distribution`, **`actions`**:`Tensor`)\n",
       "\n",
       "Calculate the log-probability of an action under a policy.\n",
       "\n",
       "Args:\n",
       "- policy (torch.distributions.Distribution): The policy distribution over input state.\n",
       "- actions (torch.Tensor): Actions to take log probability of.\n",
       "\n",
       "Returns:\n",
       "- log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CategoricalPolicy.logprob_from_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CategoricalPolicy.action_distribution\" class=\"doc_header\"><code>CategoricalPolicy.action_distribution</code><a href=\"__main__.py#L29\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CategoricalPolicy.action_distribution</code>(**`x`**:`Tensor`)\n",
       "\n",
       "Defines action distribution conditioned on input state.\n",
       "\n",
       "Args:\n",
       "- x(torch.Tensor): input state\n",
       "\n",
       "Returns:\n",
       "- Categorical distribution: Policy over the action space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CategoricalPolicy.action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class GaussianPolicy(Actor):\n",
    "    r\"\"\"\n",
    "    A class for a Gaussian Policy network. Used in continuous action space environments. The policy is an `MLP`.\n",
    "\n",
    "    Args:\n",
    "    - state_features (int): Dimensionality of the state space.\n",
    "    - action_dim (int): Dimensionality of the action space.\n",
    "    - hidden_sizes (list or tuple): Hidden layer sizes.\n",
    "    - activation (Function): Activation function for the network.\n",
    "    - out_activation (Function): Output activation function for the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_features: int,\n",
    "        action_dim: int,\n",
    "        hidden_sizes: Union[List, Tuple],\n",
    "        activation: Callable,\n",
    "        out_activation: Callable,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = MLP(\n",
    "            [state_features] + list(hidden_sizes) + [action_dim],\n",
    "            activations=activation,\n",
    "            out_act=out_activation,\n",
    "        )\n",
    "\n",
    "        self.logstd = nn.Parameter(-0.5 * torch.ones(action_dim, dtype=torch.float32))\n",
    "\n",
    "    def action_distribution(self, states):\n",
    "        \"\"\"\n",
    "        Defines action distribution conditioned on input state.\n",
    "\n",
    "        Args:\n",
    "        - x(torch.Tensor): input state\n",
    "\n",
    "        Returns:\n",
    "        - Normal distribution: Policy over the action space.\n",
    "        \"\"\"\n",
    "        mus = self.net(states)\n",
    "        std = torch.exp(self.logstd)\n",
    "        return torch.distributions.Normal(mus, std)\n",
    "\n",
    "    def logprob_from_distribution(self, policy, actions):\n",
    "        \"\"\"\n",
    "        Calculate the log-probability of an action under a policy.\n",
    "\n",
    "        Args:\n",
    "        - policy (torch.distributions.Distribution): The policy distribution over input state.\n",
    "        - actions (torch.Tensor): Actions to take log probability of.\n",
    "\n",
    "        Returns:\n",
    "        - log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution.\n",
    "        \"\"\"\n",
    "        return policy.log_prob(actions).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"GaussianPolicy\" class=\"doc_header\"><code>class</code> <code>GaussianPolicy</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>GaussianPolicy</code>(**`state_features`**:`int`, **`action_dim`**:`int`, **`hidden_sizes`**:`Union`\\[`List`\\[`T`\\], `Tuple`\\], **`activation`**:`Callable`, **`out_activation`**:`Callable`) :: [`Actor`](/rl_bolts/neuralnets#Actor)\n",
       "\n",
       "A class for a Gaussian Policy network. Used in continuous action space environments. The policy is an [`MLP`](/rl_bolts/neuralnets#MLP).\n",
       "\n",
       "Args:\n",
       "- state_features (int): Dimensionality of the state space.\n",
       "- action_dim (int): Dimensionality of the action space.\n",
       "- hidden_sizes (list or tuple): Hidden layer sizes.\n",
       "- activation (Function): Activation function for the network.\n",
       "- out_activation (Function): Output activation function for the network."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaussianPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaussianPolicy.action_distribution\" class=\"doc_header\"><code>GaussianPolicy.action_distribution</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaussianPolicy.action_distribution</code>(**`states`**)\n",
       "\n",
       "Defines action distribution conditioned on input state.\n",
       "\n",
       "Args:\n",
       "- x(torch.Tensor): input state\n",
       "\n",
       "Returns:\n",
       "- Normal distribution: Policy over the action space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaussianPolicy.action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GaussianPolicy.logprob_from_distribution\" class=\"doc_header\"><code>GaussianPolicy.logprob_from_distribution</code><a href=\"__main__.py#L46\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GaussianPolicy.logprob_from_distribution</code>(**`policy`**, **`actions`**)\n",
       "\n",
       "Calculate the log-probability of an action under a policy.\n",
       "\n",
       "Args:\n",
       "- policy (torch.distributions.Distribution): The policy distribution over input state.\n",
       "- actions (torch.Tensor): Actions to take log probability of.\n",
       "\n",
       "Returns:\n",
       "- log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GaussianPolicy.logprob_from_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class ActorCritic(nn.Module):\n",
    "    r\"\"\"\n",
    "    An Actor Critic class for Policy Gradient algorithms.\n",
    "\n",
    "    Has built-in capability to work with continuous (gym.spaces.Box) and discrete (gym.spaces.Discrete) action spaces.\n",
    "    The policy and value function are both `MLP`.\n",
    "\n",
    "    If working with a different action space,\n",
    "    the user can pass in a custom policy class for that action space as an argument.\n",
    "\n",
    "    Args:\n",
    "    - state_features (int): Dimensionality of the state space.\n",
    "    - action_space (gym.spaces.Space): Action space of the environment.\n",
    "    - hidden_sizes (list or tuple): Hidden layer sizes.\n",
    "    - activation (Function): Activation function for the network.\n",
    "    - out_activation (Function): Output activation function for the network.\n",
    "    - policy (nn.Module): Custom policy class for an environment where the action space is not gym.spaces.Box or gym.spaces.Discrete\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_features: int,\n",
    "        action_space: int,\n",
    "        hidden_sizes: Optional[Union[Tuple, List]] = (32, 32),\n",
    "        activation: Optional[Callable] = torch.tanh,\n",
    "        out_activation: Optional[Callable] = None,\n",
    "        policy: Optional[nn.Module] = None,\n",
    "    ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        obs_dim = state_features\n",
    "\n",
    "        if isinstance(action_space, gym.spaces.Discrete):\n",
    "            act_dim = action_space.n\n",
    "            pol = CategoricalPolicy\n",
    "\n",
    "        elif isinstance(action_space, gym.spaces.Box):\n",
    "            act_dim = action_space.shape[0]\n",
    "            pol = GaussianPolicy\n",
    "        else:\n",
    "            act_dim = action_space\n",
    "            pol = policy\n",
    "\n",
    "        self.policy = pol(\n",
    "            obs_dim,\n",
    "            act_dim,\n",
    "            hidden_sizes,\n",
    "            activation,\n",
    "            out_activation\n",
    "        )\n",
    "\n",
    "        self.value_f = MLP(\n",
    "            [state_features] + list(hidden_sizes) + [1],\n",
    "            activations=activation,\n",
    "            out_squeeze=True,\n",
    "        )\n",
    "\n",
    "    def step(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get action, action log probability, and value estimate for an input state.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): input state.\n",
    "\n",
    "        Returns:\n",
    "        - action (torch.Tensor): Action chosen by the policy.\n",
    "        - logp_action (torch.Tensor): Log probability of that action chosen by the policy.\n",
    "        - value (torch.Tensor): Value estimate of the current state.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            policy = self.policy.action_distribution(x)\n",
    "            action = policy.sample()\n",
    "            logp_action = self.policy.logprob_from_distribution(policy, action)\n",
    "            value = self.value_f(x)\n",
    "        return action, logp_action, value\n",
    "\n",
    "    def act(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Similar to `step`, but get only the action.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): input state\n",
    "\n",
    "        Returns:\n",
    "        - action (torch.Tensor): Action chosen by the policy.\n",
    "        \"\"\"\n",
    "        return self.step(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"ActorCritic\" class=\"doc_header\"><code>class</code> <code>ActorCritic</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>ActorCritic</code>(**`state_features`**:`int`, **`action_space`**:`int`, **`hidden_sizes`**:`Union`\\[`Tuple`, `List`\\[`T`\\], `NoneType`\\]=*`(32, 32)`*, **`activation`**:`Optional`\\[`Callable`\\]=*`'tanh'`*, **`out_activation`**:`Optional`\\[`Callable`\\]=*`None`*, **`policy`**:`Optional`\\[`Module`\\]=*`None`*) :: `Module`\n",
       "\n",
       "An Actor Critic class for Policy Gradient algorithms.\n",
       "\n",
       "Has built-in capability to work with continuous (gym.spaces.Box) and discrete (gym.spaces.Discrete) action spaces.\n",
       "The policy and value function are both [`MLP`](/rl_bolts/neuralnets#MLP).\n",
       "\n",
       "If working with a different action space,\n",
       "the user can pass in a custom policy class for that action space as an argument.\n",
       "\n",
       "Args:\n",
       "- state_features (int): Dimensionality of the state space.\n",
       "- action_space (gym.spaces.Space): Action space of the environment.\n",
       "- hidden_sizes (list or tuple): Hidden layer sizes.\n",
       "- activation (Function): Activation function for the network.\n",
       "- out_activation (Function): Output activation function for the network.\n",
       "- policy (nn.Module): Custom policy class for an environment where the action space is not gym.spaces.Box or gym.spaces.Discrete"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActorCritic.step\" class=\"doc_header\"><code>ActorCritic.step</code><a href=\"__main__.py#L60\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActorCritic.step</code>(**`x`**:`Tensor`)\n",
       "\n",
       "Get action, action log probability, and value estimate for an input state.\n",
       "\n",
       "Args:\n",
       "- x (torch.Tensor): input state.\n",
       "\n",
       "Returns:\n",
       "- action (torch.Tensor): Action chosen by the policy.\n",
       "- logp_action (torch.Tensor): Log probability of that action chosen by the policy.\n",
       "- value (torch.Tensor): Value estimate of the current state."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ActorCritic.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ActorCritic.act\" class=\"doc_header\"><code>ActorCritic.act</code><a href=\"__main__.py#L79\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ActorCritic.act</code>(**`x`**:`Tensor`)\n",
       "\n",
       "Similar to `step`, but get only the action.\n",
       "\n",
       "Args:\n",
       "- x (torch.Tensor): input state\n",
       "\n",
       "Returns:\n",
       "- action (torch.Tensor): Action chosen by the policy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ActorCritic.act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "class MLPQActor(nn.Module):\n",
    "    r\"\"\"\n",
    "    An actor for Q policy gradient algorithms. \n",
    "    \n",
    "    The policy is an `MLP`. \n",
    "    The output from the policy network is scaled to action space limits on the forward pass.\n",
    "\n",
    "    Args:\n",
    "    - state_features (int): Dimensionality of the state space.\n",
    "    - action_dim (int): Dimensionality of the action space.\n",
    "    - hidden_sizes (list or tuple): Hidden layer sizes.\n",
    "    - activation (Function): Activation function for the network.\n",
    "    - action_limit (float or int): Limits of the action space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_features: int,\n",
    "        action_dim: int,\n",
    "        hidden_sizes: Union[list, tuple],\n",
    "        activation: Callable,\n",
    "        action_limit: Union[float, int],\n",
    "    ):\n",
    "        super(MLPQActor, self).__init__()\n",
    "        policy_layer_sizes = [state_features] + list(hidden_sizes) + [action_dim]\n",
    "        self.policy = MLP(policy_layer_sizes, activation, torch.tanh)\n",
    "        self.action_limit = action_limit\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return output from the policy network scaled to the limits of the env action space.\n",
    "        Args:\n",
    "        - x (torch.Tensor): States from environment.\n",
    "        \n",
    "        Returns:\n",
    "        - scaled_action (torch.Tensor): Action scaled to action space limits.\n",
    "        \"\"\"\n",
    "        scaled_action = self.action_limit * self.policy(x)\n",
    "        return scaled_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"MLPQActor\" class=\"doc_header\"><code>class</code> <code>MLPQActor</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>MLPQActor</code>(**`state_features`**:`int`, **`action_dim`**:`int`, **`hidden_sizes`**:`Union`\\[`list`, `tuple`\\], **`activation`**:`Callable`, **`action_limit`**:`Union`\\[`float`, `int`\\]) :: `Module`\n",
       "\n",
       "An actor for Q policy gradient algorithms. \n",
       "\n",
       "The policy is an [`MLP`](/rl_bolts/neuralnets#MLP). \n",
       "The output from the policy network is scaled to action space limits on the forward pass.\n",
       "\n",
       "Args:\n",
       "- state_features (int): Dimensionality of the state space.\n",
       "- action_dim (int): Dimensionality of the action space.\n",
       "- hidden_sizes (list or tuple): Hidden layer sizes.\n",
       "- activation (Function): Activation function for the network.\n",
       "- action_limit (float or int): Limits of the action space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLPQActor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MLPQActor.forward\" class=\"doc_header\"><code>MLPQActor.forward</code><a href=\"__main__.py#L31\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>MLPQActor.forward</code>(**`x`**:`Tensor`)\n",
       "\n",
       "Return output from the policy network scaled to the limits of the env action space.\n",
       "Args:\n",
       "- x (torch.Tensor): States from environment.\n",
       "\n",
       "Returns:\n",
       "- scaled_action (torch.Tensor): Action scaled to action space limits."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLPQActor.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class MLPQFunction(nn.Module):\n",
    "    r\"\"\"\n",
    "    A Q function network for Q policy gradient methods. \n",
    "\n",
    "    The Q function is an `MLP`. It always takes in a (state, action) pair and returns a Q-value estimate for that pair.\n",
    "\n",
    "    Args:\n",
    "    - state_features (int): Dimensionality of the state space.\n",
    "    - action_dim (int): Dimensionality of the action space.\n",
    "    - hidden_sizes (list or tuple): Hidden layer sizes.\n",
    "    - activation (Function): Activation function for the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_features: int,\n",
    "        action_dim: int,\n",
    "        hidden_sizes: Union[tuple, list],\n",
    "        activation: Callable,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.qfunc = MLP(\n",
    "            [state_features + action_dim] + list(hidden_sizes) + [1], activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, a: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return Q-value estimate for state, action pair (x, a).\n",
    "        \n",
    "        Args:\n",
    "        - x (torch.Tensor): Environment state.\n",
    "        - a (torch.Tensor): Action taken by the policy.\n",
    "        \n",
    "        Returns:\n",
    "        - q (torch.Tensor): Q-value estimate for state action pair.\n",
    "        \"\"\"\n",
    "        q = self.qfunc(torch.cat([x, a], dim=-1))\n",
    "        return torch.squeeze(q, -1)  # Critical to ensure q has right shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"MLPQFunction\" class=\"doc_header\"><code>class</code> <code>MLPQFunction</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>MLPQFunction</code>(**`state_features`**:`int`, **`action_dim`**:`int`, **`hidden_sizes`**:`Union`\\[`tuple`, `list`\\], **`activation`**:`Callable`) :: `Module`\n",
       "\n",
       "A Q function network for Q policy gradient methods. \n",
       "\n",
       "The Q function is an [`MLP`](/rl_bolts/neuralnets#MLP). It always takes in a (state, action) pair and returns a Q-value estimate for that pair.\n",
       "\n",
       "Args:\n",
       "- state_features (int): Dimensionality of the state space.\n",
       "- action_dim (int): Dimensionality of the action space.\n",
       "- hidden_sizes (list or tuple): Hidden layer sizes.\n",
       "- activation (Function): Activation function for the network."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLPQFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"MLPQFunction.forward\" class=\"doc_header\"><code>MLPQFunction.forward</code><a href=\"__main__.py#L27\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>MLPQFunction.forward</code>(**`x`**:`Tensor`, **`a`**:`Tensor`)\n",
       "\n",
       "Return Q-value estimate for state, action pair (x, a).\n",
       "\n",
       "Args:\n",
       "- x (torch.Tensor): Environment state.\n",
       "- a (torch.Tensor): Action taken by the policy.\n",
       "\n",
       "Returns:\n",
       "- q (torch.Tensor): Q-value estimate for state action pair."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MLPQFunction.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted 04_losses.ipynb.\n",
      "Converted 05_env_wrappers.ipynb.\n",
      "Converted 06_loops.ipynb.\n",
      "Converted 07_algorithms.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
