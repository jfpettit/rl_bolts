{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# losses\n",
    "\n",
    "> This module defines losses for a variety of RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def actor_critic_value_loss(value_estimates: torch.Tensor, env_returns: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss for an actor-critic value function.\n",
    "    \n",
    "    Is just Mean-Squared-Error between the value estimates and the real returns.\n",
    "    \n",
    "    Args:\n",
    "    - value_estimates (torch.Tensor): Estimates of state-value from the critic network.\n",
    "    - env_returns (torch.Tensor): Real returns from the environment.\n",
    "    \n",
    "    Returns:\n",
    "    - value_loss (torch.Tensor): MSE loss betwen the estimates and real returns.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(value_estimates, env_returns)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"actor_critic_value_loss\" class=\"doc_header\"><code>actor_critic_value_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>actor_critic_value_loss</code>(**`value_estimates`**:`Tensor`, **`env_returns`**:`Tensor`)\n",
       "\n",
       "Loss for an actor-critic value function.\n",
       "\n",
       "Is just Mean-Squared-Error between the value estimates and the real returns.\n",
       "\n",
       "Args:\n",
       "- value_estimates (torch.Tensor): Estimates of state-value from the critic network.\n",
       "- env_returns (torch.Tensor): Real returns from the environment.\n",
       "\n",
       "Returns:\n",
       "- value_loss (torch.Tensor): MSE loss betwen the estimates and real returns."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(actor_critic_value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "vest = torch.tensor([0.])\n",
    "rtrue = torch.tensor([0.])\n",
    "assert actor_critic_value_loss(vest, rtrue) is not None, \"Val loss fails to return proper value\"\n",
    "assert actor_critic_value_loss(vest, rtrue) == torch.Tensor([0.]), \"Val loss is calculated incorrectly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def reinforce_policy_loss(logps: torch.Tensor, env_returns: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Reinforce Policy gradient loss. $-(log(\\pi(a | s)) * R_t)$\n",
    "\n",
    "    Args:\n",
    "    - logps (PyTorch Tensor): Action log probabilities.\n",
    "    - env_returns (PyTorch Tensor): Returns from the environment.\n",
    "    \n",
    "    Returns:\n",
    "    - reinforce_loss (torch.Tensor): REINFORCE loss term.\n",
    "    \"\"\"\n",
    "    reinforce_loss = -(logps * env_returns).mean()\n",
    "    return reinforce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"reinforce_policy_loss\" class=\"doc_header\"><code>reinforce_policy_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>reinforce_policy_loss</code>(**`logps`**:`Tensor`, **`env_returns`**:`Tensor`)\n",
       "\n",
       "Reinforce Policy gradient loss. $-(log(\\pi(a | s)) * R_t)$\n",
       "\n",
       "Args:\n",
       "- logps (PyTorch Tensor): Action log probabilities.\n",
       "- env_returns (PyTorch Tensor): Returns from the environment.\n",
       "\n",
       "Returns:\n",
       "- reinforce_loss (torch.Tensor): REINFORCE loss term."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(reinforce_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tmp_logp = torch.tensor([-0.3])\n",
    "tmp_ret = torch.tensor([10.])\n",
    "assert reinforce_policy_loss(tmp_logp, tmp_ret) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def a2c_policy_loss(logps: torch.Tensor, advs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss function for an A2C policy. $-(logp(\\pi(a|s)) * A_t)$\n",
    "    \n",
    "    Args:\n",
    "    - logps (torch.Tensor): Log-probabilities of selected actions.\n",
    "    - advs (torch.Tensor): Advantage estimates of selected actions.\n",
    "    \n",
    "    Returns:\n",
    "    - a2c_loss (torch.Tensor): A2C loss term.\n",
    "    \"\"\"\n",
    "    a2c_loss = -(logps * advs).mean()\n",
    "    return a2c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"a2c_policy_loss\" class=\"doc_header\"><code>a2c_policy_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>a2c_policy_loss</code>(**`logps`**:`Tensor`, **`advs`**:`Tensor`)\n",
       "\n",
       "Loss function for an A2C policy. $-(logp(\\pi(a|s)) * A_t)$\n",
       "\n",
       "Args:\n",
       "- logps (torch.Tensor): Log-probabilities of selected actions.\n",
       "- advs (torch.Tensor): Advantage estimates of selected actions.\n",
       "\n",
       "Returns:\n",
       "- a2c_loss (torch.Tensor): A2C loss term."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(a2c_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert a2c_policy_loss(tmp_logp, tmp_ret) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def ppo_clip_policy_loss(\n",
    "    logps: torch.Tensor, \n",
    "    logps_old: torch.Tensor, \n",
    "    advs: torch.Tensor, \n",
    "    clipratio: Optional[float] = 0.2\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss function for a PPO-clip policy. \n",
    "    See paper for full loss function math: https://arxiv.org/abs/1707.06347\n",
    "    \n",
    "    Args:\n",
    "    - logps (torch.Tensor): Action log-probabilities under the current policy.\n",
    "    - logps_old (torch.Tensor): Action log-probabilities under the old (pre-update) policy.\n",
    "    - advs (torch.Tensor): Advantage estimates for the actions taken.\n",
    "    - clipratio (float): Clipping parameter for PPO-clip loss. In general, is fine with being left as default.\n",
    "    \n",
    "    Returns:\n",
    "    - ppo_loss (torch.Tensor): Loss term for PPO agent.\n",
    "    - kl (torch.Tensor): KL-divergence estimate between new and old policies.\n",
    "    \"\"\"\n",
    "    policy_ratio = torch.exp(logps - logps_old)\n",
    "    clipped_adv = torch.clamp(policy_ratio, 1 - clipratio, 1 + clipratio) * advs\n",
    "    ppo_loss = -(torch.min(policy_ratio * advs, clipped_adv)).mean()\n",
    "\n",
    "    kl = (logps_old - logps).mean().item()\n",
    "    return ppo_loss, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ppo_clip_policy_loss\" class=\"doc_header\"><code>ppo_clip_policy_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ppo_clip_policy_loss</code>(**`logps`**:`Tensor`, **`logps_old`**:`Tensor`, **`advs`**:`Tensor`, **`clipratio`**:`Optional`\\[`float`\\]=*`0.2`*)\n",
       "\n",
       "Loss function for a PPO-clip policy. \n",
       "See paper for full loss function math: https://arxiv.org/abs/1707.06347\n",
       "\n",
       "Args:\n",
       "- logps (torch.Tensor): Action log-probabilities under the current policy.\n",
       "- logps_old (torch.Tensor): Action log-probabilities under the old (pre-update) policy.\n",
       "- advs (torch.Tensor): Advantage estimates for the actions taken.\n",
       "- clipratio (float): Clipping parameter for PPO-clip loss. In general, is fine with being left as default.\n",
       "\n",
       "Returns:\n",
       "- ppo_loss (torch.Tensor): Loss term for PPO agent.\n",
       "- kl (torch.Tensor): KL-divergence estimate between new and old policies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ppo_clip_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tmp_logp_old = torch.tensor([-0.2])\n",
    "assert ppo_clip_policy_loss(tmp_logp, tmp_logp_old, tmp_ret) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def ddpg_policy_loss(states: torch.Tensor, qfunc: nn.Module, policy: nn.Module):\n",
    "    \"\"\"\n",
    "    Policy loss function for DDPG agent. See the paper: https://arxiv.org/abs/1509.02971\n",
    "    \n",
    "    Args:\n",
    "    - states (torch.Tensor): States to get Q-policy estimates for.\n",
    "    - qfunc (nn.Module): Q-function network.\n",
    "    - policy (nn.Module): Policy network.\n",
    "    \n",
    "    Returns:\n",
    "    - q_policy_loss (torch.Tensor): Loss term for DDPG policy.\n",
    "    \"\"\"\n",
    "    q_pi = qfunc(states, policy(states))\n",
    "    q_policy_loss = -q_pi.mean()\n",
    "    return q_policy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ddpg_policy_loss\" class=\"doc_header\"><code>ddpg_policy_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ddpg_policy_loss</code>(**`states`**:`Tensor`, **`qfunc`**:`Module`, **`policy`**:`Module`)\n",
       "\n",
       "Policy loss function for DDPG agent. See the paper: https://arxiv.org/abs/1509.02971\n",
       "\n",
       "Args:\n",
       "- states (torch.Tensor): States to get Q-policy estimates for.\n",
       "- qfunc (nn.Module): Q-function network.\n",
       "- policy (nn.Module): Policy network.\n",
       "\n",
       "Returns:\n",
       "- q_policy_loss (torch.Tensor): Loss term for DDPG policy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ddpg_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def ddpg_qfunc_loss(\n",
    "    data: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "    qfunc: nn.Module, \n",
    "    qfunc_target: nn.Module, \n",
    "    policy_target: nn.Module,\n",
    "    gamma: Optional[float] = 0.99\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Loss for a DDPG Q-function. See the paper: https://arxiv.org/abs/1509.02971\n",
    "    \n",
    "    Args:\n",
    "    - data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the\n",
    "    following: (states, next_states, actions, rewards, dones).\n",
    "    - qfunc (nn.Module): Q-function network being trained.\n",
    "    - qfunc_target (nn.Module): Q-function target network.\n",
    "    - policy_target (nn.Module): Policy target network.\n",
    "    - gamma (float): Discount factor.\n",
    "    \n",
    "    Returns:\n",
    "    - loss_q (torch.Tensor): DDPG loss for the Q-function.\n",
    "    - loss_info (dict): Dictionary containing useful loss info for logging.\n",
    "    \"\"\"\n",
    "    o, o2, a, r, d = data \n",
    "\n",
    "    q = qfunc(o, a)\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = qfunc_target(o2, policy_target(o2))\n",
    "        backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q = ((q - backup) ** 2).mean()\n",
    "\n",
    "    # Useful info for logging\n",
    "    loss_info = dict(MeanQValues=q.mean().detach().numpy())\n",
    "\n",
    "    return loss_q, loss_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ddpg_qfunc_loss\" class=\"doc_header\"><code>ddpg_qfunc_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ddpg_qfunc_loss</code>(**`data`**:`Tuple`\\[`Tensor`, `Tensor`, `Tensor`, `Tensor`, `Tensor`\\], **`qfunc`**:`Module`, **`qfunc_target`**:`Module`, **`policy_target`**:`Module`, **`gamma`**:`Optional`\\[`float`\\]=*`0.99`*)\n",
       "\n",
       "Loss for a DDPG Q-function. See the paper: https://arxiv.org/abs/1509.02971\n",
       "\n",
       "Args:\n",
       "- data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the\n",
       "following: (states, next_states, actions, rewards, dones).\n",
       "- qfunc (nn.Module): Q-function network being trained.\n",
       "- qfunc_target (nn.Module): Q-function target network.\n",
       "- policy_target (nn.Module): Policy target network.\n",
       "- gamma (float): Discount factor.\n",
       "\n",
       "Returns:\n",
       "- loss_q (torch.Tensor): DDPG loss for the Q-function.\n",
       "- loss_info (dict): Dictionary containing useful loss info for logging."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ddpg_qfunc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def td3_policy_loss(states: torch.Tensor, qfunc: nn.Module, policy: nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate policy loss for TD3 agent. See paper here: https://arxiv.org/abs/1802.09477\n",
    "    \n",
    "    Args:\n",
    "    - states (torch.Tensor): Input states to get policy loss for.\n",
    "    - qfunc (torch.Tensor): TD3 q-function network.\n",
    "    - policy (torch.Tensor): Policy network.\n",
    "    \n",
    "    Returns:\n",
    "    - q_policy_loss (torch.Tensor): The TD3 policy loss term.\n",
    "    \"\"\"\n",
    "    q1_pi = qfunc1(states, policy(states))\n",
    "    q_policy_loss = -q1_pi.mean()\n",
    "    return q_policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"td3_policy_loss\" class=\"doc_header\"><code>td3_policy_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>td3_policy_loss</code>(**`states`**:`Tensor`, **`qfunc`**:`Module`, **`policy`**:`Module`)\n",
       "\n",
       "Calculate policy loss for TD3 agent. See paper here: https://arxiv.org/abs/1802.09477\n",
       "\n",
       "Args:\n",
       "- states (torch.Tensor): Input states to get policy loss for.\n",
       "- qfunc (torch.Tensor): TD3 q-function network.\n",
       "- policy (torch.Tensor): Policy network.\n",
       "\n",
       "Returns:\n",
       "- q_policy_loss (torch.Tensor): The TD3 policy loss term."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(td3_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def td3_qfunc_loss(\n",
    "    data: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    qfunc1: nn.Module,\n",
    "    qfunc2: nn.Module,\n",
    "    qfunc1_target: nn.Module,\n",
    "    qfunc2_target: nn.Module,\n",
    "    policy: nn.Module,\n",
    "    act_limit: Union[float, int],\n",
    "    target_noise: Optional[float] = 0.2,\n",
    "    noise_clip: Optional[float] = 0.5,\n",
    "    gamma: Optional[float] = 0.99,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculate Q-function loss for TD3 agent. See paper here: https://arxiv.org/abs/1802.09477\n",
    "    \n",
    "    Args:\n",
    "    - data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the\n",
    "    following: (states, next_states, actions, rewards, dones).\n",
    "    - qfunc1 (nn.Module): First Q-function network being trained.\n",
    "    - qfunc2 (nn.Module): Other Q-function network being trained.\n",
    "    - qfunc1_target (nn.Module): First Q-function target network.\n",
    "    - qfunc2_target (nn.Module): Other Q-function target network.\n",
    "    - policy (nn.Module): Policy network.\n",
    "    - act_limit (float or int): Action limit from the environment.\n",
    "    - target_noise (float): Noise to apply to policy target network.\n",
    "    - noise_clip (float): Clip the noise within + and - this range.\n",
    "    - gamma (float): Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "    - loss_q (torch.Tensor): TD3 loss for the Q-function.\n",
    "    - loss_info (dict): Dictionary containing useful loss info for logging.\n",
    "    \"\"\"\n",
    "    o, a, r, o2, d = data\n",
    "\n",
    "    q1 = qfunc1(o, a)\n",
    "    q2 = qfunc2(o, a)\n",
    "\n",
    "    # Bellman backup for Q functions\n",
    "    with torch.no_grad():\n",
    "        pi_targ = policy(o2)\n",
    "\n",
    "        # Target policy smoothing\n",
    "        epsilon = torch.randn_like(pi_targ) * target_noise\n",
    "        epsilon = torch.clamp(epsilon, -noise_clip, noise_clip)\n",
    "        a2 = pi_targ + epsilon\n",
    "        a2 = torch.clamp(a2, -act_limit, act_limit)\n",
    "\n",
    "        # Target Q-values\n",
    "        q1_pi_targ = qfunc1_target(o2, a2)\n",
    "        q2_pi_targ = qfunc2_target(o2, a2)\n",
    "        q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "        backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q1 = ((q1 - backup) ** 2).mean()\n",
    "    loss_q2 = ((q2 - backup) ** 2).mean()\n",
    "    loss_q = loss_q1 + loss_q2\n",
    "\n",
    "    # Useful info for logging\n",
    "    loss_info = dict(Q1Values=q1.detach().numpy(), Q2Values=q2.detach().numpy())\n",
    "\n",
    "    return loss_q, loss_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"td3_qfunc_loss\" class=\"doc_header\"><code>td3_qfunc_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>td3_qfunc_loss</code>(**`data`**:`Tuple`\\[`Tensor`, `Tensor`, `Tensor`, `Tensor`, `Tensor`\\], **`qfunc1`**:`Module`, **`qfunc2`**:`Module`, **`qfunc1_target`**:`Module`, **`qfunc2_target`**:`Module`, **`policy`**:`Module`, **`act_limit`**:`Union`\\[`float`, `int`\\], **`target_noise`**:`Optional`\\[`float`\\]=*`0.2`*, **`noise_clip`**:`Optional`\\[`float`\\]=*`0.5`*, **`gamma`**:`Optional`\\[`float`\\]=*`0.99`*)\n",
       "\n",
       "Calculate Q-function loss for TD3 agent. See paper here: https://arxiv.org/abs/1802.09477\n",
       "\n",
       "Args:\n",
       "- data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the\n",
       "following: (states, next_states, actions, rewards, dones).\n",
       "- qfunc1 (nn.Module): First Q-function network being trained.\n",
       "- qfunc2 (nn.Module): Other Q-function network being trained.\n",
       "- qfunc1_target (nn.Module): First Q-function target network.\n",
       "- qfunc2_target (nn.Module): Other Q-function target network.\n",
       "- policy (nn.Module): Policy network.\n",
       "- act_limit (float or int): Action limit from the environment.\n",
       "- target_noise (float): Noise to apply to policy target network.\n",
       "- noise_clip (float): Clip the noise within + and - this range.\n",
       "- gamma (float): Gamma discount factor.\n",
       "\n",
       "Returns:\n",
       "- loss_q (torch.Tensor): TD3 loss for the Q-function.\n",
       "- loss_info (dict): Dictionary containing useful loss info for logging."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(td3_qfunc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def sac_policy_loss(\n",
    "    states: torch.Tensor, \n",
    "    qfunc1: nn.Module, \n",
    "    qfunc2: nn.Module, \n",
    "    policy: nn.Module,\n",
    "    alpha: Optional[float] = 0.2\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculate policy loss for Soft-Actor Critic agent. See paper here: https://arxiv.org/abs/1801.01290\n",
    "    \n",
    "    Args:\n",
    "    - states (torch.Tensor): Input states for the policy.\n",
    "    - qfunc1 (nn.Module): First Q-function in SAC agent.\n",
    "    - qfunc2 (nn.Module): Second Q-function in SAC agent.\n",
    "    - policy (nn.Module): Policy network.\n",
    "    - alpha (float): alpha factor for entropy-regularized policy loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss_policy (torch.Tensor): The policy loss term.\n",
    "    - policy_info (dict): Useful logging info for the policy.\n",
    "    \"\"\"\n",
    "    o = states\n",
    "    pi, logp_pi = policy(o)\n",
    "    q1_pi = qfunc1(o, pi)\n",
    "    q2_pi = qfunc2(o, pi)\n",
    "    q_pi = torch.min(q1_pi, q2_pi)\n",
    "\n",
    "    # Entropy-regularized policy loss\n",
    "    loss_policy = (alpha * logp_pi - q_pi).mean()\n",
    "\n",
    "    # Useful info for logging\n",
    "    policy_info = dict(PolicyLogP=logp_pi.detach().numpy())\n",
    "\n",
    "    return loss_policy, policy_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"sac_policy_loss\" class=\"doc_header\"><code>sac_policy_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>sac_policy_loss</code>(**`states`**:`Tensor`, **`qfunc1`**:`Module`, **`qfunc2`**:`Module`, **`policy`**:`Module`, **`alpha`**:`Optional`\\[`float`\\]=*`0.2`*)\n",
       "\n",
       "Calculate policy loss for Soft-Actor Critic agent. See paper here: https://arxiv.org/abs/1801.01290\n",
       "\n",
       "Args:\n",
       "- states (torch.Tensor): Input states for the policy.\n",
       "- qfunc1 (nn.Module): First Q-function in SAC agent.\n",
       "- qfunc2 (nn.Module): Second Q-function in SAC agent.\n",
       "- policy (nn.Module): Policy network.\n",
       "- alpha (float): alpha factor for entropy-regularized policy loss.\n",
       "\n",
       "Returns:\n",
       "- loss_policy (torch.Tensor): The policy loss term.\n",
       "- policy_info (dict): Useful logging info for the policy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(sac_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def sac_qfunc_loss(\n",
    "    data,\n",
    "    qfunc1: nn.Module,\n",
    "    qfunc2: nn.Module,\n",
    "    qfunc1_target: nn.Module,\n",
    "    qfunc2_target: nn.Module,\n",
    "    policy: nn.Module,\n",
    "    gamma: Optional[float] = 0.99,\n",
    "    alpha: Optional[float] = 0.2\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Q-function loss for Soft-Actor Critic agent.\n",
    "    \n",
    "    Args:\n",
    "    - data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the\n",
    "    following: (states, next_states, actions, rewards, dones).\n",
    "    - qfunc1 (nn.Module): First Q-function network being trained.\n",
    "    - qfunc2 (nn.Module): Other Q-function network being trained.\n",
    "    - qfunc1_target (nn.Module): First Q-function target network.\n",
    "    - qfunc2_target (nn.Module): Other Q-function target network.\n",
    "    - policy (nn.Module): Policy network.\n",
    "    - gamma (float): Gamma discount factor.\n",
    "    - alpha (float): Loss term alpha factor.\n",
    "    \n",
    "    Returns:\n",
    "    - loss_q (torch.Tensor): SAC loss for the Q-function.\n",
    "    - loss_info (dict): Dictionary containing useful loss info for logging.\n",
    "    \"\"\"\n",
    "    o, a, r, o2, d = data\n",
    "\n",
    "    q1 = qfunc1(o, a)\n",
    "    q2 = qfunc2(o, a)\n",
    "\n",
    "    # Bellman backup for Q functions\n",
    "    with torch.no_grad():\n",
    "        # Target actions come from *current* policy\n",
    "        a2, logp_a2 = policy(o2)\n",
    "\n",
    "        # Target Q-values\n",
    "        q1_pi_targ = qfunc1_target(o2, a2)\n",
    "        q2_pi_targ = qfunc2_target(o2, a2)\n",
    "        q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
    "        backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q1 = ((q1 - backup) ** 2).mean()\n",
    "    loss_q2 = ((q2 - backup) ** 2).mean()\n",
    "    loss_q = loss_q1 + loss_q2\n",
    "\n",
    "    # Useful info for logging\n",
    "    q_info = dict(Q1Values=q1.detach().numpy(), Q2Values=q2.detach().numpy())\n",
    "\n",
    "    return loss_q, q_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"sac_qfunc_loss\" class=\"doc_header\"><code>sac_qfunc_loss</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>sac_qfunc_loss</code>(**`data`**, **`qfunc1`**:`Module`, **`qfunc2`**:`Module`, **`qfunc1_target`**:`Module`, **`qfunc2_target`**:`Module`, **`policy`**:`Module`, **`gamma`**:`Optional`\\[`float`\\]=*`0.99`*, **`alpha`**:`Optional`\\[`float`\\]=*`0.2`*)\n",
       "\n",
       "Q-function loss for Soft-Actor Critic agent.\n",
       "\n",
       "Args:\n",
       "- data (tuple of torch.Tensor): input data batch. Contains 5 PyTorch Tensors. The tensors contain the\n",
       "following: (states, next_states, actions, rewards, dones).\n",
       "- qfunc1 (nn.Module): First Q-function network being trained.\n",
       "- qfunc2 (nn.Module): Other Q-function network being trained.\n",
       "- qfunc1_target (nn.Module): First Q-function target network.\n",
       "- qfunc2_target (nn.Module): Other Q-function target network.\n",
       "- policy (nn.Module): Policy network.\n",
       "- gamma (float): Gamma discount factor.\n",
       "- alpha (float): Loss term alpha factor.\n",
       "\n",
       "Returns:\n",
       "- loss_q (torch.Tensor): SAC loss for the Q-function.\n",
       "- loss_info (dict): Dictionary containing useful loss info for logging."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(sac_qfunc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted 04_losses.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
