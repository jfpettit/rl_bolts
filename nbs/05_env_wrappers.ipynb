{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp env_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# env_wrappers\n",
    "\n",
    "> Here we provide a useful set of environment wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class ToTorchWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Environment wrapper for converting actions from torch.Tensors to np.array and converting observations from np.array to\n",
    "    torch.Tensors.\n",
    "    \n",
    "    Args:\n",
    "    - env (gym.Env): Environment to wrap. Should be a subclass of gym.Env and follow the OpenAI Gym API.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "    def reset(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        \n",
    "        Returns:\n",
    "        - tensor_obs (torch.Tensor): output of reset as PyTorch Tensor.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset(*args, **kwargs)\n",
    "        tensor_obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        return tensor_obs\n",
    "    \n",
    "    def step(self, action: torch.Tensor, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Execute environment step.\n",
    "        \n",
    "        Converts from torch.Tensor action and returns observations as a torch.Tensor.\n",
    "        \n",
    "        Returns:\n",
    "        - tensor_obs (torch.Tensor): Next observations as pytorch tensor.\n",
    "        - reward (float or int): The reward earned at the current timestep.\n",
    "        - done (bool): Whether the episode is in a terminal state.\n",
    "        - infos (dict): The info dict from the environment.\n",
    "        \"\"\"\n",
    "        \n",
    "        action = self.action2np(action)\n",
    "        obs, reward, done, infos = self.env.step(action, *args, **kwargs)\n",
    "        tensor_obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        return tensor_obs, reward, done, infos\n",
    "    \n",
    "    def action2np(self, action: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Convert torch.Tensor action to NumPy.\n",
    "        \n",
    "        Args:\n",
    "        - action (torch.Tensor): The action to convert.\n",
    "        \n",
    "        Returns:\n",
    "        - np_act (np.array or int): The action converted to numpy.\n",
    "        \"\"\"\n",
    "        if isinstance(self.action_space, gym.spaces.Discrete):\n",
    "            action_map = lambda action: int(action.squeeze().numpy())\n",
    "        if isinstance(self.action_space, gym.spaces.Box):\n",
    "            action_map = lambda action: action.numpy()\n",
    "            \n",
    "        np_act = action_map(action)\n",
    "        return np_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"ToTorchWrapper\" class=\"doc_header\"><code>class</code> <code>ToTorchWrapper</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>ToTorchWrapper</code>(**`env`**:`Env`) :: `Wrapper`\n",
       "\n",
       "Environment wrapper for converting actions from torch.Tensors to np.array and converting observations from np.array to\n",
       "torch.Tensors.\n",
       "\n",
       "Args:\n",
       "- env (gym.Env): Environment to wrap. Should be a subclass of gym.Env and follow the OpenAI Gym API."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ToTorchWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ToTorchWrapper.reset\" class=\"doc_header\"><code>ToTorchWrapper.reset</code><a href=\"__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ToTorchWrapper.reset</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Reset the environment.\n",
       "\n",
       "Returns:\n",
       "- tensor_obs (torch.Tensor): output of reset as PyTorch Tensor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ToTorchWrapper.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ToTorchWrapper.step\" class=\"doc_header\"><code>ToTorchWrapper.step</code><a href=\"__main__.py#L26\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ToTorchWrapper.step</code>(**`action`**:`Tensor`, **\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Execute environment step.\n",
       "\n",
       "Converts from torch.Tensor action and returns observations as a torch.Tensor.\n",
       "\n",
       "Returns:\n",
       "- tensor_obs (torch.Tensor): Next observations as pytorch tensor.\n",
       "- reward (float or int): The reward earned at the current timestep.\n",
       "- done (bool): Whether the episode is in a terminal state.\n",
       "- infos (dict): The info dict from the environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ToTorchWrapper.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ToTorchWrapper.action2np\" class=\"doc_header\"><code>ToTorchWrapper.action2np</code><a href=\"__main__.py#L44\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ToTorchWrapper.action2np</code>(**`action`**:`Tensor`)\n",
       "\n",
       "Convert torch.Tensor action to NumPy.\n",
       "\n",
       "Args:\n",
       "- action (torch.Tensor): The action to convert.\n",
       "\n",
       "Returns:\n",
       "- np_act (np.array or int): The action converted to numpy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ToTorchWrapper.action2np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage of the `ToTorchWrapper` is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial obs: tensor([ 0.0439, -0.0047,  0.0234,  0.0489])\n",
      "stepped once: (tensor([ 0.0438,  0.1901,  0.0243, -0.2363]), 1.0, False, {})\n",
      "\n",
      "Entering interaction loop! \n",
      "\n",
      "Random policy got 25.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 16.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 16.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 12.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 11.0 reward!\n",
      "Starting new episode.\n",
      "\n",
      "Interaction loop ended! Got reward 20.0 before episode was cut off.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = ToTorchWrapper(env)\n",
    "obs = env.reset()\n",
    "print(\"initial obs:\", obs)\n",
    "action = env.action_space.sample()\n",
    "# need to convert action to PyTorch Tensor because ToTorchWrapper expects actions as Tensors.\n",
    "# normally you would not need to do this, your PyTorch NN actor will output a Tensor by default.\n",
    "action = torch.as_tensor(action, dtype=torch.float32)\n",
    "stepped = env.step(action)\n",
    "print(\"stepped once:\", stepped)\n",
    "\n",
    "print(\"\\nEntering interaction loop! \\n\")\n",
    "# interaction loop\n",
    "obs = env.reset()\n",
    "ret = 0\n",
    "for i in range(100):\n",
    "    action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    ret += reward\n",
    "    if done:\n",
    "        print(f\"Random policy got {ret} reward!\")\n",
    "        obs = env.reset()\n",
    "        ret = 0\n",
    "        if i < 99:\n",
    "            print(\"Starting new episode.\")\n",
    "    if i == 99:\n",
    "        print(f\"\\nInteraction loop ended! Got reward {ret} before episode was cut off.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = ToTorchWrapper(env)\n",
    "obs = env.reset()\n",
    "assert type(obs) == torch.Tensor\n",
    "action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "step_out = env.step(action)\n",
    "assert type(step_out[0]) == torch.Tensor\n",
    "\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "env = ToTorchWrapper(env)\n",
    "obs = env.reset()\n",
    "assert type(obs) == torch.Tensor\n",
    "action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "step_out = env.step(action)\n",
    "assert type(step_out[0]) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class StateNormalizeWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Environment wrapper for normalizing states.\n",
    "    \n",
    "    Args:\n",
    "    - env (gym.Env): Environment to wrap.\n",
    "    - beta (float): Beta parameter for running mean and variance calculation.\n",
    "    - eps (float): Parameter to avoid division by zero in case variance goes to zero.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, beta: Optional[float] = 0.99, eps: Optional[float] = 1e-8):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.mean = np.zeros(self.observation_space.shape)\n",
    "        self.var = np.ones(self.observation_space.shape)\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        \n",
    "    def normalize(self, state: np.array):\n",
    "        \"\"\"\n",
    "        Update running mean and variance parameters and normalize input state.\n",
    "        \n",
    "        Args:\n",
    "        - state (np.array): State to normalize and to use to calculate update.\n",
    "        \n",
    "        Returns:\n",
    "        - norm_state (np.array): Normalized state.\n",
    "        \"\"\"\n",
    "        self.mean = self.beta * self.mean + (1. - self.beta) * state\n",
    "        self.var = self.beta * self.var + (1. - self.beta) * np.square(state - self.mean)\n",
    "        norm_state = (state - self.mean) / (np.sqrt(self.var) + self.eps)\n",
    "        return norm_state\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Reset environment and return normalized state.\n",
    "        \n",
    "        Returns:\n",
    "        - norm_state (np.array): Normalized state.\n",
    "        \"\"\"\n",
    "        state = self.env.reset()\n",
    "        norm_state = self.normalize(state)\n",
    "        return norm_state\n",
    "    \n",
    "    def step(self, action: Union[np.array, int, float], *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Step environment and normalize state.\n",
    "        \n",
    "        Args:\n",
    "        - action (np.array or int or float): Action to use to step the environment.\n",
    "        \n",
    "        Returns:\n",
    "        - norm_state (np.array): Normalized state.\n",
    "        - reward (int or float): Reward earned at step.\n",
    "        - done (bool): Whether the episode is over.\n",
    "        - infos (dict): Any infos from the environment.\n",
    "        \"\"\"\n",
    "        state, reward, done, infos = self.env.step(action, *args, **kwargs)\n",
    "        norm_state = self.normalize(state)\n",
    "        return norm_state, reward, done, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Testing needed for StateNormalizeWrapper. At present, use `ToTorchWrapper` for guaranteed working.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"StateNormalizeWrapper\" class=\"doc_header\"><code>class</code> <code>StateNormalizeWrapper</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>StateNormalizeWrapper</code>(**`env`**:`Env`, **`beta`**:`Optional`\\[`float`\\]=*`0.99`*, **`eps`**:`Optional`\\[`float`\\]=*`1e-08`*) :: `Wrapper`\n",
       "\n",
       "Environment wrapper for normalizing states.\n",
       "\n",
       "Args:\n",
       "- env (gym.Env): Environment to wrap.\n",
       "- beta (float): Beta parameter for running mean and variance calculation.\n",
       "- eps (float): Parameter to avoid division by zero in case variance goes to zero."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StateNormalizeWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StateNormalizeWrapper.reset\" class=\"doc_header\"><code>StateNormalizeWrapper.reset</code><a href=\"__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StateNormalizeWrapper.reset</code>(**\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Reset environment and return normalized state.\n",
       "\n",
       "Returns:\n",
       "- norm_state (np.array): Normalized state."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StateNormalizeWrapper.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StateNormalizeWrapper.normalize\" class=\"doc_header\"><code>StateNormalizeWrapper.normalize</code><a href=\"__main__.py#L22\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StateNormalizeWrapper.normalize</code>(**`state`**:`array`)\n",
       "\n",
       "Update running mean and variance parameters and normalize input state.\n",
       "\n",
       "Args:\n",
       "- state (np.array): State to normalize and to use to calculate update.\n",
       "\n",
       "Returns:\n",
       "- norm_state (np.array): Normalized state."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StateNormalizeWrapper.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StateNormalizeWrapper.step\" class=\"doc_header\"><code>StateNormalizeWrapper.step</code><a href=\"__main__.py#L48\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StateNormalizeWrapper.step</code>(**`action`**:`Union`\\[`array`, `int`, `float`\\], **\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Step environment and normalize state.\n",
       "\n",
       "Args:\n",
       "- action (np.array or int or float): Action to use to step the environment.\n",
       "\n",
       "Returns:\n",
       "- norm_state (np.array): Normalized state.\n",
       "- reward (int or float): Reward earned at step.\n",
       "- done (bool): Whether the episode is over.\n",
       "- infos (dict): Any infos from the environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StateNormalizeWrapper.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demonstration of using the `StateNormalizeWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial obs: [ 0.01758044 -0.04254612 -0.02514053  0.01284619]\n",
      "stepped once: (array([ 0.01663708,  0.15312245, -0.02475622, -0.28764562]), 1.0, False, {})\n",
      "\n",
      "Entering interaction loop! \n",
      "\n",
      "Random policy got 10.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 11.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 20.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 22.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 12.0 reward!\n",
      "Starting new episode.\n",
      "Random policy got 22.0 reward!\n",
      "Starting new episode.\n",
      "\n",
      "Interaction loop ended! Got reward 3.0 before episode was cut off.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = StateNormalizeWrapper(env)\n",
    "obs = env.reset()\n",
    "print(\"initial obs:\", obs)\n",
    "# the StateNormalizeWrapper expects NumPy arrays, so there is no need to convert action to PyTorch Tensor.\n",
    "action = env.action_space.sample()\n",
    "stepped = env.step(action)\n",
    "print(\"stepped once:\", stepped)\n",
    "\n",
    "print(\"\\nEntering interaction loop! \\n\")\n",
    "# interaction loop\n",
    "obs = env.reset()\n",
    "ret = 0\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    ret += reward\n",
    "    if done:\n",
    "        print(f\"Random policy got {ret} reward!\")\n",
    "        obs = env.reset()\n",
    "        ret = 0\n",
    "        if i < 99:\n",
    "            print(\"Starting new episode.\")\n",
    "    if i == 99:\n",
    "        print(f\"\\nInteraction loop ended! Got reward {ret} before episode was cut off.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = StateNormalizeWrapper(env)\n",
    "assert env.reset() is not None\n",
    "action = env.action_space.sample()\n",
    "assert env.step(action) is not None\n",
    "env = ToTorchWrapper(env)\n",
    "assert env.reset() is not None\n",
    "assert type(env.reset()) == torch.Tensor\n",
    "action = env.action_space.sample()\n",
    "t_action = torch.as_tensor(action, dtype=torch.float32)\n",
    "assert env.step(t_action) is not None\n",
    "assert type(env.step(t_action)[0]) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class RewardScalerWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A class for reward scaling over training.\n",
    "    \n",
    "    Calculates running mean and standard deviation of observed rewards and scales the rewards using the variance.\n",
    "    \n",
    "    Computes: $r_t / (\\sigma + eps)$\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, beta: Optional[float] = 0.99, eps: Optional[float] = 1e-8):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.var = 1\n",
    "        self.mean = 0\n",
    "        \n",
    "    def scale(self, reward: Union[int, float]):\n",
    "        \"\"\"\n",
    "        Update running mean and variance for rewards, scale reward using the variance.\n",
    "        \n",
    "        Args:\n",
    "        - reward (int or float): reward to scale.\n",
    "        \n",
    "        Returns:\n",
    "        - scaled_rew (float): reward scaled using variance.\n",
    "        \"\"\"\n",
    "        self.mean = self.beta * self.mean + (1. - self.beta) * reward\n",
    "        self.var = self.beta * self.var + (1. - self.beta) * np.square(reward - self.mean)\n",
    "        \n",
    "        scaled_rew = (reward - self.mean) / (np.sqrt(self.var) + self.eps)\n",
    "        \n",
    "        return scaled_rew\n",
    "    \n",
    "    def step(self, action, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Step the environment and scale the reward.\n",
    "        \n",
    "        Args:\n",
    "        - action (np.array or int or float): Action to use to step the environment.\n",
    "        \n",
    "        Returns:\n",
    "        - state (np.array): Next state from environment.\n",
    "        - scaled_rew (float): reward scaled using the variance.\n",
    "        - done (bool): Indicates whether the episode is over.\n",
    "        - infos (dict): Any information from the environment.\n",
    "        \"\"\"\n",
    "        state, reward, done, infos = self.env.step(action, *args, **kwargs)\n",
    "        scaled_rew = self.scale(reward)\n",
    "        return state, scaled_rew, done, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RewardScalerWrapper(env)\n",
    "assert env.reset() is not None\n",
    "action = env.action_space.sample()\n",
    "assert env.step(action) is not None\n",
    "assert type(env.step(action)[0]) == np.ndarray\n",
    "env = StateNormalizeWrapper(env)\n",
    "assert env.reset() is not None\n",
    "action = env.action_space.sample()\n",
    "assert env.step(action) is not None\n",
    "assert type(env.step(action)[0]) == np.ndarray\n",
    "env = ToTorchWrapper(env)\n",
    "assert env.reset() is not None\n",
    "assert type(env.reset()) == torch.Tensor\n",
    "action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "assert env.step(action) is not None\n",
    "assert type(env.step(action)[0]) == torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Testing needed for RewardScalerWrapper. At present, use `ToTorchWrapper` for guaranteed working.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"RewardScalerWrapper\" class=\"doc_header\"><code>class</code> <code>RewardScalerWrapper</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>RewardScalerWrapper</code>(**`env`**:`Env`, **`beta`**:`Optional`\\[`float`\\]=*`0.99`*, **`eps`**:`Optional`\\[`float`\\]=*`1e-08`*) :: `Wrapper`\n",
       "\n",
       "A class for reward scaling over training.\n",
       "\n",
       "Calculates running mean and standard deviation of observed rewards and scales the rewards using the variance.\n",
       "\n",
       "Computes: $r_t / (\\sigma + eps)$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RewardScalerWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"RewardScalerWrapper.scale\" class=\"doc_header\"><code>RewardScalerWrapper.scale</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>RewardScalerWrapper.scale</code>(**`reward`**:`Union`\\[`int`, `float`\\])\n",
       "\n",
       "Update running mean and variance for rewards, scale reward using the variance.\n",
       "\n",
       "Args:\n",
       "- reward (int or float): reward to scale.\n",
       "\n",
       "Returns:\n",
       "- scaled_rew (float): reward scaled using variance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RewardScalerWrapper.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"RewardScalerWrapper.step\" class=\"doc_header\"><code>RewardScalerWrapper.step</code><a href=\"__main__.py#L36\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>RewardScalerWrapper.step</code>(**`action`**, **\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Step the environment and scale the reward.\n",
       "\n",
       "Args:\n",
       "- action (np.array or int or float): Action to use to step the environment.\n",
       "\n",
       "Returns:\n",
       "- state (np.array): Next state from environment.\n",
       "- scaled_rew (float): reward scaled using the variance.\n",
       "- done (bool): Indicates whether the episode is over.\n",
       "- infos (dict): Any information from the environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(RewardScalerWrapper.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example usage of the RewardScalerWrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial obs: [-0.03681186 -0.01856562  0.01785368 -0.03059186]\n",
      "stepped once: (array([-0.03718318, -0.213939  ,  0.01724184,  0.26767019]), 0.9900985098023393, False, {})\n",
      "\n",
      "Entering interaction loop! \n",
      "\n",
      "Random policy got 25.870551503555898 reward!\n",
      "Starting new episode.\n",
      "Random policy got 6.588056312915322 reward!\n",
      "Starting new episode.\n",
      "Random policy got 26.21475981461599 reward!\n",
      "Starting new episode.\n",
      "Random policy got 6.0767512893302875 reward!\n",
      "Starting new episode.\n",
      "\n",
      "Interaction loop ended! Got reward 2.871941385677035 before episode was cut off.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = RewardScalerWrapper(env)\n",
    "obs = env.reset()\n",
    "print(\"initial obs:\", obs)\n",
    "action = env.action_space.sample()\n",
    "stepped = env.step(action)\n",
    "print(\"stepped once:\", stepped)\n",
    "\n",
    "print(\"\\nEntering interaction loop! \\n\")\n",
    "# interaction loop\n",
    "obs = env.reset()\n",
    "ret = 0\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    ret += reward\n",
    "    if done:\n",
    "        print(f\"Random policy got {ret} reward!\")\n",
    "        obs = env.reset()\n",
    "        ret = 0\n",
    "        if i < 99:\n",
    "            print(\"Starting new episode.\")\n",
    "    if i == 99:\n",
    "        print(f\"\\nInteraction loop ended! Got reward {ret} before episode was cut off.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Wrappers \n",
    "\n",
    "All of these wrappers can be composed together! Simply be sure to call the `ToTorchWrapper` last, because the others expect NumPy arrays as input, and the `ToTorchWrapper` converts outputs to PyTorch tensors. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After wrapping with StateNormalizeWrapper, output is still a NumPy array: [-0.0072026  -0.00074714  0.01404444  0.01655632]\n",
      "After wrapping with RewardScalerWrapper, output is still a NumPy array: [-0.01601177 -0.03326409 -0.02039952  0.02392616]\n",
      "But after wrapping with ToTorchWrapper, output is now a PyTorch Tensor: tensor([-0.0485,  0.0209, -0.0479, -0.0501])\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = StateNormalizeWrapper(env)\n",
    "print(f\"After wrapping with StateNormalizeWrapper, output is still a NumPy array: {env.reset()}\")\n",
    "env = RewardScalerWrapper(env)\n",
    "print(f\"After wrapping with RewardScalerWrapper, output is still a NumPy array: {env.reset()}\")\n",
    "env = ToTorchWrapper(env)\n",
    "print(f\"But after wrapping with ToTorchWrapper, output is now a PyTorch Tensor: {env.reset()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class BestPracticesWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    This wrapper combines the wrappers which we think (from experience and from reading papers/blogs and watching lectures)\n",
    "    constitute best practices.\n",
    "    \n",
    "    At the moment it combines the wrappers below in the order listed:\n",
    "    1. StateNormalizeWrapper\n",
    "    2. RewardScalerWrapper\n",
    "    3. ToTorchWrapper\n",
    "    \n",
    "    Args:\n",
    "    - env (gym.Env): Environment to wrap.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        env = StateNormalizeWrapper(env)\n",
    "        env = RewardScalerWrapper(env)\n",
    "        self.env = ToTorchWrapper(env)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment.\n",
    "        \n",
    "        Returns:\n",
    "        - obs (torch.Tensor): Starting observations from the environment.\n",
    "        \"\"\"\n",
    "        obs = self.env.reset()\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Step the environment forward using input action.\n",
    "        \n",
    "        Args:\n",
    "        - action (torch.Tensor): Action to step the environment with.\n",
    "        \n",
    "        Returns:\n",
    "        - obs (torch.Tensor): Next step observations.\n",
    "        - reward (int or float): Reward for the last timestep.\n",
    "        - done (bool): Whether the episode is over.\n",
    "        - infos (dict): Dictionary of any info from the environment.\n",
    "        \"\"\"\n",
    "        obs, reward, done, infos = self.env.step(action, *args, **kwargs)\n",
    "        return obs, reward, done, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = BestPracticesWrapper(env)\n",
    "assert env.reset() is not None\n",
    "assert type(env.reset()) == torch.Tensor\n",
    "action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "stepped = env.step(action)\n",
    "assert stepped is not None\n",
    "assert type(stepped[0]) == torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Testing needed for BestPracticesWrapper. At present, use `ToTorchWrapper` for guaranteed working.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"BestPracticesWrapper\" class=\"doc_header\"><code>class</code> <code>BestPracticesWrapper</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>BestPracticesWrapper</code>(**`env`**:`Env`) :: `Wrapper`\n",
       "\n",
       "This wrapper combines the wrappers which we think (from experience and from reading papers/blogs and watching lectures)\n",
       "constitute best practices.\n",
       "\n",
       "At the moment it combines the wrappers below in the order listed:\n",
       "1. StateNormalizeWrapper\n",
       "2. RewardScalerWrapper\n",
       "3. ToTorchWrapper\n",
       "\n",
       "Args:\n",
       "- env (gym.Env): Environment to wrap."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BestPracticesWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BestPracticesWrapper.reset\" class=\"doc_header\"><code>BestPracticesWrapper.reset</code><a href=\"__main__.py#L22\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BestPracticesWrapper.reset</code>()\n",
       "\n",
       "Reset environment.\n",
       "\n",
       "Returns:\n",
       "- obs (torch.Tensor): Starting observations from the environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BestPracticesWrapper.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BestPracticesWrapper.step\" class=\"doc_header\"><code>BestPracticesWrapper.step</code><a href=\"__main__.py#L32\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BestPracticesWrapper.step</code>(**`action`**, **\\*`args`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Step the environment forward using input action.\n",
       "\n",
       "Args:\n",
       "- action (torch.Tensor): Action to step the environment with.\n",
       "\n",
       "Returns:\n",
       "- obs (torch.Tensor): Next step observations.\n",
       "- reward (int or float): Reward for the last timestep.\n",
       "- done (bool): Whether the episode is over.\n",
       "- infos (dict): Dictionary of any info from the environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BestPracticesWrapper.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a usage example of the `BestPracticesWrapper`. It is used in the same way as the `ToTorchWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial obs: tensor([-0.0468, -0.0292, -0.0462,  0.0099])\n",
      "stepped once: (tensor([-0.0471, -0.2234, -0.0458,  0.2874]), 0.9900985098023393, False, {})\n",
      "\n",
      "Entering interaction loop! \n",
      "\n",
      "Random policy got 22.00490875509153 reward!\n",
      "Starting new episode.\n",
      "Random policy got 22.999644404672914 reward!\n",
      "Starting new episode.\n",
      "Random policy got 16.764618492994995 reward!\n",
      "Starting new episode.\n",
      "Random policy got 4.907345113364475 reward!\n",
      "Starting new episode.\n",
      "\n",
      "Interaction loop ended! Got reward 0.9455435399706331 before episode was cut off.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = BestPracticesWrapper(env)\n",
    "obs = env.reset()\n",
    "print(\"initial obs:\", obs)\n",
    "action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "stepped = env.step(action)\n",
    "print(\"stepped once:\", stepped)\n",
    "\n",
    "print(\"\\nEntering interaction loop! \\n\")\n",
    "# interaction loop\n",
    "obs = env.reset()\n",
    "ret = 0\n",
    "for i in range(100):\n",
    "    action = torch.as_tensor(env.action_space.sample(), dtype=torch.float32)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    ret += reward\n",
    "    if done:\n",
    "        print(f\"Random policy got {ret} reward!\")\n",
    "        obs = env.reset()\n",
    "        ret = 0\n",
    "        if i < 99:\n",
    "            print(\"Starting new episode.\")\n",
    "    if i == 99:\n",
    "        print(f\"\\nInteraction loop ended! Got reward {ret} before episode was cut off.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted 04_losses.ipynb.\n",
      "Converted 05_env_wrappers.ipynb.\n",
      "Converted 06_loops.ipynb.\n",
      "Converted 07_algorithms.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
