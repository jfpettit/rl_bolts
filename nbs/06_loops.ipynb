{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loops\n",
    "\n",
    "> This module will include some useful interaction loops for types of RL agents. It'll be updated over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import gym\n",
    "import numpy as np\n",
    "from rl_bolts import buffers, env_wrappers, neuralnets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def polgrad_interaction_loop(\n",
    "    env: gym.Env, \n",
    "    agent: nn.Module, \n",
    "    buffer: buffers.PGBuffer, \n",
    "    num_interactions: int = 4000, \n",
    "    horizon: int = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Interaction loop for actor-critic policy gradient agent.\n",
    "    \n",
    "    This loop does not handle converting between PyTorch Tensors and NumPy arrays. So either your env should first be wrapped\n",
    "    in `ToTorchWrapper` or your agent should accept and return NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "    - env (gym.Env): Environment to run in. \n",
    "    - agent (nn.Module): Agent to run within the environment, generates actions, values, and logprobs at each step.\n",
    "    - buffer (rl_bolts.buffers.PGBuffer-like): Buffer object with same API and function signatures as the PGBuffer.\n",
    "    - num_interactions (int): How many interactions to collect in the environment.\n",
    "    - horizon (int): Maximum allowed episode length.\n",
    "    \n",
    "    Returns:\n",
    "    - buffer (rl_bolts.buffers.PGBuffer-like): Buffer filled with interactions.\n",
    "    - infos (dict): Dictionary of reward and episode length statistics.\n",
    "    - env_infos (list of dicts): List of all info dicts from the environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    env_infos = []\n",
    "    \n",
    "    rets = []\n",
    "    lens = []\n",
    "    \n",
    "    ret = 0\n",
    "    length = 0\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    for i in range(num_interactions):\n",
    "        action, logp, value = agent.step(obs)\n",
    "        \n",
    "        next_obs, reward, done, env_info = env.step(action)\n",
    "        env_infos.append(env_info)\n",
    "        \n",
    "        buffer.store(\n",
    "            obs,\n",
    "            action,\n",
    "            reward,\n",
    "            value,\n",
    "            logp\n",
    "        )\n",
    "        \n",
    "        ret += reward\n",
    "        length += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        timeup = length == horizon\n",
    "        over = done or timeup\n",
    "        epoch_ended = i == num_interactions - 1\n",
    "        \n",
    "        if over or epoch_ended:\n",
    "            if timeup or epoch_ended:\n",
    "                with torch.no_grad():\n",
    "                    last_val = agent.value_f(obs)\n",
    "                \n",
    "            else:\n",
    "                last_val = 0\n",
    "            \n",
    "            buffer.finish_path(last_val)\n",
    "                \n",
    "            if over:\n",
    "                rets.append(ret)\n",
    "                lens.append(length)\n",
    "            \n",
    "            obs, ret, length = env.reset(), 0, 0\n",
    "            \n",
    "    infos = {\n",
    "        \"MeanEpReturn\": np.mean(rets),\n",
    "        \"StdEpReturn\": np.std(rets),\n",
    "        \"MaxEpReturn\": np.max(rets),\n",
    "        \"MinEpReturn\": np.min(rets),\n",
    "        \"MeanEpLength\": np.mean(lens),\n",
    "        \"StdEpLength\": np.std(lens)\n",
    "    }\n",
    "        \n",
    "    return buffer, infos, env_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"polgrad_interaction_loop\" class=\"doc_header\"><code>polgrad_interaction_loop</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>polgrad_interaction_loop</code>(**`env`**:`Env`, **`agent`**:`Module`, **`buffer`**:[`PGBuffer`](/rl_bolts/buffers#PGBuffer), **`num_interactions`**:`int`=*`4000`*, **`horizon`**:`int`=*`1000`*)\n",
       "\n",
       "Interaction loop for actor-critic policy gradient agent.\n",
       "\n",
       "This loop does not handle converting between PyTorch Tensors and NumPy arrays. So either your env should first be wrapped\n",
       "in [`ToTorchWrapper`](/rl_bolts/env_wrappers#ToTorchWrapper) or your agent should accept and return NumPy arrays.\n",
       "\n",
       "Args:\n",
       "- env (gym.Env): Environment to run in. \n",
       "- agent (nn.Module): Agent to run within the environment, generates actions, values, and logprobs at each step.\n",
       "- buffer (rl_bolts.buffers.PGBuffer-like): Buffer object with same API and function signatures as the PGBuffer.\n",
       "- num_interactions (int): How many interactions to collect in the environment.\n",
       "- horizon (int): Maximum allowed episode length.\n",
       "\n",
       "Returns:\n",
       "- buffer (rl_bolts.buffers.PGBuffer-like): Buffer filled with interactions.\n",
       "- infos (dict): Dictionary of reward and episode length statistics.\n",
       "- env_infos (list of dicts): List of all info dicts from the environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(polgrad_interaction_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate hypothetical usage of the interaction loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanEpReturn: 25.477707006369428\n",
      "StdEpReturn: 14.071059873100223\n",
      "MaxEpReturn: 100.0\n",
      "MinEpReturn: 9.0\n",
      "MeanEpLength: 25.477707006369428\n",
      "StdEpLength: 14.071059873100223\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\") # make the environment\n",
    "env = env_wrappers.ToTorchWrapper(env) # wrap it for conversion to/from torch.Tensors\n",
    "agent = neuralnets.ActorCritic( # make the actor-critic agent\n",
    "    env.observation_space.shape[0],\n",
    "    env.action_space,\n",
    ")\n",
    "buf = buffers.PGBuffer(env.observation_space.shape, env.action_space.shape, 4000) # create empty buffer\n",
    "full_buf, infos, env_infos = polgrad_interaction_loop(env, agent, buf) # run loop, fills buffer\n",
    "for k, v in infos.items(): # print loop stats\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted 04_losses.ipynb.\n",
      "Converted 05_env_wrappers.ipynb.\n",
      "Converted 06_loops.ipynb.\n",
      "Converted 07_algorithms.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
