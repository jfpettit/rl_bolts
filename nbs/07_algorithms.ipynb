{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithms\n",
    "\n",
    "> This module contains algorithms we choose to implement and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rl_bolts import neuralnets as nns\n",
    "from rl_bolts import losses as l\n",
    "from rl_bolts.env_wrappers import BestPracticesWrapper, ToTorchWrapper, StateNormalizeWrapper\n",
    "from rl_bolts.buffers import PGBuffer\n",
    "from rl_bolts.datasets import PolicyGradientRLDataset\n",
    "from rl_bolts.loops import polgrad_interaction_loop\n",
    "import rl_bolts.utils as utils\n",
    "import pytorch_lightning as pl\n",
    "from argparse import Namespace\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class PPO(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of the Proximal Policy Optimization (PPO) algorithm. See the paper: https://arxiv.org/abs/1707.06347\n",
    "    \n",
    "    It is a PyTorch Lightning Module. See their docs: https://pytorch-lightning.readthedocs.io/en/latest/\n",
    "    \n",
    "    Args:\n",
    "    - env (str): Environment to run in. Handles vector observation environments with either gym.spaces.Box or gym.spaces.Discrete\n",
    "    action space.\n",
    "    - hidden_sizes (tuple): Hidden layer sizes for actor-critic network.\n",
    "    - gamma (float): Discount factor.\n",
    "    - lam (float): Lambda factor for GAE-Lambda calculation.\n",
    "    - clipratio (float): Clip ratio for PPO-clip objective.\n",
    "    - train_iters (int): How many steps to take over the latest data batch.\n",
    "    - batch_size (int): How many interactions to collect per update.\n",
    "    - pol_lr (float): Learning rate for the policy optimizer.\n",
    "    - val_lr (float): Learning rate for the value optimizer.\n",
    "    - maxkl (float): Max allowed KL divergence between policy updates.\n",
    "    - seed (int): Random seed for pytorch and numpy\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: str, \n",
    "        hidden_sizes: Optional[tuple] = (32, 32), \n",
    "        gamma: Optional[float] = 0.99, \n",
    "        lam: Optional[float] = 0.97,\n",
    "        clipratio: Optional[float] = 0.2,\n",
    "        train_iters: Optional[int] = 80,\n",
    "        batch_size: Optional[int] = 4000,\n",
    "        pol_lr: Optional[float] = 3e-4,\n",
    "        val_lr: Optional[float] = 1e-3,\n",
    "        maxkl: Optional[float] = 0.01,\n",
    "        seed: Optional[int] = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        \n",
    "        hparams = Namespace(\n",
    "             **{\n",
    "                'env':env, \n",
    "                'hidden_sizes':hidden_sizes, \n",
    "                'gamma':gamma, \n",
    "                'lam':lam, \n",
    "                'clipratio':clipratio, \n",
    "                'train_iters':train_iters,\n",
    "                'batch_size':batch_size,\n",
    "                'pol_lr':pol_lr,\n",
    "                'val_lr':val_lr,\n",
    "                'maxkl':maxkl\n",
    "             }\n",
    "        ) \n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        env = gym.make(env)\n",
    "        self.env = ToTorchWrapper(env)\n",
    "         \n",
    "        self.actor_critic = nns.ActorCritic(\n",
    "            self.env.observation_space.shape[0],\n",
    "            self.env.action_space,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "        )\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.clipratio = clipratio \n",
    "        self.train_iters = train_iters \n",
    "        self.batch_size = batch_size \n",
    "        self.pol_lr = pol_lr\n",
    "        self.val_lr = val_lr\n",
    "        self.maxkl = maxkl\n",
    "        \n",
    "        self.tracker_dict = {}\n",
    "        \n",
    "        self.buffer = PGBuffer(\n",
    "            self.env.observation_space.shape,\n",
    "            self.env.action_space.shape,\n",
    "            size = self.batch_size,\n",
    "            gamma = self.gamma\n",
    "        )\n",
    "        \n",
    "        self.inner_loop()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        self.policy_optimizer = torch.optim.Adam(self.actor_critic.policy.parameters(), lr=self.pol_lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.actor_critic.value_f.parameters(), lr=self.val_lr)\n",
    "        return self.policy_optimizer, self.value_optimizer\n",
    "    \n",
    "    def forward(self, x, a = None):\n",
    "        out = self.actor_critic(x, a)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        states, actions, advs, rets, logps_old = batch\n",
    "        \n",
    "        if optimizer_idx == 0:\n",
    "            stops = 0\n",
    "            stopslst = []\n",
    "            policy, logps = self.actor_critic.policy(states, actions)\n",
    "            pol_loss_old, kl = l.ppo_clip_policy_loss(logps, logps_old, advs, clipratio=self.clipratio)\n",
    "\n",
    "            for i in range(self.train_iters):\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy, logps = self.actor_critic.policy(states, actions)\n",
    "                pol_loss, kl = l.ppo_clip_policy_loss(logps, logps_old, advs, clipratio=self.clipratio)\n",
    "                if kl > 1.5 * self.maxkl:\n",
    "                    stops += 1\n",
    "                    stopslst.append(i)\n",
    "                    break\n",
    "                pol_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "            log = {\n",
    "                \"PolicyLoss\": pol_loss_old.item(),\n",
    "                \"DeltaPolLoss\": (pol_loss - pol_loss_old).item(),\n",
    "                \"KL\": kl,\n",
    "                \"Entropy\": policy.entropy().mean().item(),\n",
    "                \"TimesEarlyStopped\": stops,\n",
    "                \"AvgEarlyStopStep\": np.mean(stopslst) if len(stopslst) > 0 else 0\n",
    "            }\n",
    "            loss = pol_loss_old\n",
    "\n",
    "        elif optimizer_idx == 1:\n",
    "            values_old = self.actor_critic.value_f(states)\n",
    "            val_loss_old = l.actor_critic_value_loss(values_old, rets)\n",
    "            for i in range(self.train_iters):\n",
    "                self.value_optimizer.zero_grad()\n",
    "                values = self.actor_critic.value_f(states)\n",
    "                val_loss = l.actor_critic_value_loss(values, rets)\n",
    "                val_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "            delta_val_loss = (val_loss - val_loss_old).item()\n",
    "            log = {\"ValueLoss\": val_loss_old.item(), \"DeltaValLoss\": delta_val_loss}\n",
    "            loss = val_loss\n",
    "\n",
    "        self.tracker_dict.update(log)\n",
    "        return {\"loss\": loss, \"log\": log, \"progress_bar\": log}\n",
    "    \n",
    "    def inner_loop(self) -> None:\n",
    "        buffer, infos, _ = polgrad_interaction_loop(self.env, self.actor_critic, self.buffer, self.batch_size) \n",
    "        self.data = buffer.get()\n",
    "        self.tracker_dict.update(infos)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        utils.printdict(self.tracker_dict)\n",
    "        self.tracker_dict = {}\n",
    "        self.inner_loop()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset = PolicyGradientRLDataset(self.data)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, sampler=None, num_workers=4)\n",
    "        return dataloader\n",
    "    \n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"PPO\" class=\"doc_header\"><code>class</code> <code>PPO</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>PPO</code>(**`env`**:`str`, **`hidden_sizes`**:`Optional`\\[`tuple`\\]=*`(32, 32)`*, **`gamma`**:`Optional`\\[`float`\\]=*`0.99`*, **`lam`**:`Optional`\\[`float`\\]=*`0.97`*, **`clipratio`**:`Optional`\\[`float`\\]=*`0.2`*, **`train_iters`**:`Optional`\\[`int`\\]=*`80`*, **`batch_size`**:`Optional`\\[`int`\\]=*`4000`*, **`pol_lr`**:`Optional`\\[`float`\\]=*`0.0003`*, **`val_lr`**:`Optional`\\[`float`\\]=*`0.001`*, **`maxkl`**:`Optional`\\[`float`\\]=*`0.01`*, **`seed`**:`Optional`\\[`int`\\]=*`0`*) :: `LightningModule`\n",
       "\n",
       "Implementation of the Proximal Policy Optimization (PPO) algorithm. See the paper: https://arxiv.org/abs/1707.06347\n",
       "\n",
       "It is a PyTorch Lightning Module. See their docs: https://pytorch-lightning.readthedocs.io/en/latest/\n",
       "\n",
       "Args:\n",
       "- env (str): Environment to run in. Handles vector observation environments with either gym.spaces.Box or gym.spaces.Discrete\n",
       "action space.\n",
       "- hidden_sizes (tuple): Hidden layer sizes for actor-critic network.\n",
       "- gamma (float): Discount factor.\n",
       "- lam (float): Lambda factor for GAE-Lambda calculation.\n",
       "- clipratio (float): Clip ratio for PPO-clip objective.\n",
       "- train_iters (int): How many steps to take over the latest data batch.\n",
       "- batch_size (int): How many interactions to collect per update.\n",
       "- pol_lr (float): Learning rate for the policy optimizer.\n",
       "- val_lr (float): Learning rate for the value optimizer.\n",
       "- maxkl (float): Max allowed KL divergence between policy updates.\n",
       "- seed (int): Random seed for pytorch and numpy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example training PPO on the [CartPole-v1 environment](https://gym.openai.com/envs/CartPole-v1/). Since it is a [PyTorch-Lightning Module](https://pytorch-lightning.readthedocs.io/en/latest/) it is trained using their Trainer API. \n",
    "\n",
    "**Note that this PPO implementation needs to be more thoroughly benchmarked and so may be a work in progress.**\n",
    "\n",
    "The `reload_dataloaders_every_epoch` flag is needed to ensure that during each training step, the updates are computed on the latest batch of data.\n",
    "\n",
    "To see how we implement this, view the source code for the `PPO` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(\"CartPole-v1\")\n",
    "trainer = pl.Trainer(reload_dataloaders_every_epoch=True, max_epochs=25)\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted 04_losses.ipynb.\n",
      "Converted 05_env_wrappers.ipynb.\n",
      "Converted 06_loops.ipynb.\n",
      "Converted 07_algorithms.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
