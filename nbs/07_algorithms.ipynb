{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithms\n",
    "\n",
    "> This module contains algorithms we choose to implement and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rl_bolts import neuralnets as nns\n",
    "from rl_bolts import losses as l\n",
    "from rl_bolts.env_wrappers import BestPracticesWrapper, ToTorchWrapper, StateNormalizeWrapper\n",
    "from rl_bolts.buffers import PGBuffer\n",
    "from rl_bolts.datasets import PolicyGradientRLDataset\n",
    "from rl_bolts.loops import polgrad_interaction_loop\n",
    "import rl_bolts.utils as utils\n",
    "import pytorch_lightning as pl\n",
    "from argparse import Namespace\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class PPO(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of the Proximal Policy Optimization (PPO) algorithm. See the paper: https://arxiv.org/abs/1707.06347\n",
    "    \n",
    "    It is a PyTorch Lightning Module. See their docs: https://pytorch-lightning.readthedocs.io/en/latest/\n",
    "    \n",
    "    Args:\n",
    "    - env (str): Environment to run in. Handles vector observation environments with either gym.spaces.Box or gym.spaces.Discrete\n",
    "    action space.\n",
    "    - hidden_sizes (tuple): Hidden layer sizes for actor-critic network.\n",
    "    - gamma (float): Discount factor.\n",
    "    - lam (float): Lambda factor for GAE-Lambda calculation.\n",
    "    - clipratio (float): Clip ratio for PPO-clip objective.\n",
    "    - train_iters (int): How many steps to take over the latest data batch.\n",
    "    - batch_size (int): How many interactions to collect per update.\n",
    "    - pol_lr (float): Learning rate for the policy optimizer.\n",
    "    - val_lr (float): Learning rate for the value optimizer.\n",
    "    - maxkl (float): Max allowed KL divergence between policy updates.\n",
    "    - seed (int): Random seed for pytorch and numpy\n",
    "    - evaluate (bool): Whether to run eval episodes at the end of each epoch. Saves episodes using gym.wrappers.Monitor.\n",
    "    - monitor_dir (str): Directory for monitor to write to. Default is /tmp\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: str, \n",
    "        hidden_sizes: Optional[tuple] = (32, 32), \n",
    "        gamma: Optional[float] = 0.99, \n",
    "        lam: Optional[float] = 0.97,\n",
    "        clipratio: Optional[float] = 0.2,\n",
    "        train_iters: Optional[int] = 80,\n",
    "        batch_size: Optional[int] = 4000,\n",
    "        pol_lr: Optional[float] = 3e-4,\n",
    "        val_lr: Optional[float] = 1e-3,\n",
    "        maxkl: Optional[float] = 0.01,\n",
    "        seed: Optional[int] = 0,\n",
    "        evaluate: Optional[bool] = True,\n",
    "        monitor_dir: Optional[str] = 'video_results'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        \n",
    "        hparams = Namespace(\n",
    "             **{\n",
    "                'env':env, \n",
    "                'hidden_sizes':hidden_sizes, \n",
    "                'gamma':gamma, \n",
    "                'lam':lam, \n",
    "                'clipratio':clipratio, \n",
    "                'train_iters':train_iters,\n",
    "                'batch_size':batch_size,\n",
    "                'pol_lr':pol_lr,\n",
    "                'val_lr':val_lr,\n",
    "                'maxkl':maxkl\n",
    "             }\n",
    "        ) \n",
    "        \n",
    "        self.hparams = hparams\n",
    "        \n",
    "        env = gym.make(env)\n",
    "        self.env = ToTorchWrapper(env)\n",
    "         \n",
    "        self.actor_critic = nns.ActorCritic(\n",
    "            self.env.observation_space.shape[0],\n",
    "            self.env.action_space,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "        )\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.clipratio = clipratio \n",
    "        self.train_iters = train_iters \n",
    "        self.batch_size = batch_size \n",
    "        self.pol_lr = pol_lr\n",
    "        self.val_lr = val_lr\n",
    "        self.maxkl = maxkl\n",
    "        self.evaluate = evaluate\n",
    "        \n",
    "        if self.evaluate:\n",
    "            eval_env = gym.wrappers.Monitor(env, monitor_dir, force=True)\n",
    "            self.eval_env = ToTorchWrapper(eval_env)\n",
    "        \n",
    "        self.tracker_dict = {}\n",
    "        \n",
    "        self.buffer = PGBuffer(\n",
    "            self.env.observation_space.shape,\n",
    "            self.env.action_space.shape,\n",
    "            size = self.batch_size,\n",
    "            gamma = self.gamma\n",
    "        )\n",
    "        \n",
    "        self.inner_loop()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        self.policy_optimizer = torch.optim.Adam(self.actor_critic.policy.parameters(), lr=self.pol_lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.actor_critic.value_f.parameters(), lr=self.val_lr)\n",
    "        return self.policy_optimizer, self.value_optimizer\n",
    "    \n",
    "    def forward(self, x, a = None):\n",
    "        out = self.actor_critic(x, a)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        states, actions, advs, rets, logps_old = batch\n",
    "        \n",
    "        if optimizer_idx == 0:\n",
    "            stops = 0\n",
    "            stopslst = []\n",
    "            policy, logps = self.actor_critic.policy(states, actions)\n",
    "            pol_loss_old, kl = l.ppo_clip_policy_loss(logps, logps_old, advs, clipratio=self.clipratio)\n",
    "\n",
    "            for i in range(self.train_iters):\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy, logps = self.actor_critic.policy(states, actions)\n",
    "                pol_loss, kl = l.ppo_clip_policy_loss(logps, logps_old, advs, clipratio=self.clipratio)\n",
    "                if kl > 1.5 * self.maxkl:\n",
    "                    stops += 1\n",
    "                    stopslst.append(i)\n",
    "                    break\n",
    "                pol_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "            log = {\n",
    "                \"PolicyLoss\": pol_loss_old.item(),\n",
    "                \"DeltaPolLoss\": (pol_loss - pol_loss_old).item(),\n",
    "                \"KL\": kl,\n",
    "                \"Entropy\": policy.entropy().mean().item(),\n",
    "                \"TimesEarlyStopped\": stops,\n",
    "                \"AvgEarlyStopStep\": np.mean(stopslst) if len(stopslst) > 0 else 0\n",
    "            }\n",
    "            loss = pol_loss_old\n",
    "\n",
    "        elif optimizer_idx == 1:\n",
    "            values_old = self.actor_critic.value_f(states)\n",
    "            val_loss_old = l.actor_critic_value_loss(values_old, rets)\n",
    "            for i in range(self.train_iters):\n",
    "                self.value_optimizer.zero_grad()\n",
    "                values = self.actor_critic.value_f(states)\n",
    "                val_loss = l.actor_critic_value_loss(values, rets)\n",
    "                val_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "            delta_val_loss = (val_loss - val_loss_old).item()\n",
    "            log = {\"ValueLoss\": val_loss_old.item(), \"DeltaValLoss\": delta_val_loss}\n",
    "            loss = val_loss\n",
    "\n",
    "        self.tracker_dict.update(log)\n",
    "        log.update(self.tracker_dict)\n",
    "        return {\"loss\": loss, \"log\": log, \"progress_bar\": log}\n",
    "    \n",
    "    def inner_loop(self) -> None:\n",
    "        buffer, infos, _ = polgrad_interaction_loop(self.env, self.actor_critic, self.buffer, self.batch_size) \n",
    "        self.data = buffer.get()\n",
    "        self.tracker_dict.update(infos)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        utils.printdict(self.tracker_dict)\n",
    "        self.tracker_dict = {}\n",
    "        self.inner_loop()\n",
    "        self.eval_episodes(n_episodes=1)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset = PolicyGradientRLDataset(self.data)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, sampler=None, num_workers=4)\n",
    "        return dataloader\n",
    "    \n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def eval_episodes(self, n_episodes = 3):\n",
    "        if self.evaluate:\n",
    "            with torch.no_grad():\n",
    "                rets_lst = []\n",
    "                lens_lst = []\n",
    "                for i in range(n_episodes):\n",
    "                    episode_return = 0\n",
    "                    episode_length = 0\n",
    "                    obs = self.eval_env.reset()\n",
    "                    done = False\n",
    "                    while not done:\n",
    "                        action, logp, value = self.actor_critic.step(obs)\n",
    "\n",
    "                        obs, r, done, _ = self.eval_env.step(action)\n",
    "                        episode_return += r\n",
    "                        episode_length += 1\n",
    "\n",
    "                        if done:\n",
    "                            rets_lst.append(episode_return)\n",
    "                            lens_lst.append(episode_length)\n",
    "\n",
    "                            episode_return = 0\n",
    "                            episode_length = 0\n",
    "                            obs = self.eval_env.reset()\n",
    "\n",
    "                dct = {\n",
    "                    \"NumEvalEpisodes\": n_episodes,\n",
    "                    \"MeanEvalEpReturn\": np.mean(rets_lst),\n",
    "                    \"StdEvalEpReturn\": np.std(rets_lst),\n",
    "                    \"MeanEvalEpLength\": np.mean(lens_lst),\n",
    "                    \"StdEvalEpLength\": np.std(lens_lst)\n",
    "                }\n",
    "\n",
    "            self.tracker_dict.update(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"PPO\" class=\"doc_header\"><code>class</code> <code>PPO</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>PPO</code>(**`env`**:`str`, **`hidden_sizes`**:`Optional`\\[`tuple`\\]=*`(32, 32)`*, **`gamma`**:`Optional`\\[`float`\\]=*`0.99`*, **`lam`**:`Optional`\\[`float`\\]=*`0.97`*, **`clipratio`**:`Optional`\\[`float`\\]=*`0.2`*, **`train_iters`**:`Optional`\\[`int`\\]=*`80`*, **`batch_size`**:`Optional`\\[`int`\\]=*`4000`*, **`pol_lr`**:`Optional`\\[`float`\\]=*`0.0003`*, **`val_lr`**:`Optional`\\[`float`\\]=*`0.001`*, **`maxkl`**:`Optional`\\[`float`\\]=*`0.01`*, **`seed`**:`Optional`\\[`int`\\]=*`0`*, **`evaluate`**:`Optional`\\[`bool`\\]=*`True`*, **`monitor_dir`**:`Optional`\\[`str`\\]=*`'video_results'`*) :: `LightningModule`\n",
       "\n",
       "Implementation of the Proximal Policy Optimization (PPO) algorithm. See the paper: https://arxiv.org/abs/1707.06347\n",
       "\n",
       "It is a PyTorch Lightning Module. See their docs: https://pytorch-lightning.readthedocs.io/en/latest/\n",
       "\n",
       "Args:\n",
       "- env (str): Environment to run in. Handles vector observation environments with either gym.spaces.Box or gym.spaces.Discrete\n",
       "action space.\n",
       "- hidden_sizes (tuple): Hidden layer sizes for actor-critic network.\n",
       "- gamma (float): Discount factor.\n",
       "- lam (float): Lambda factor for GAE-Lambda calculation.\n",
       "- clipratio (float): Clip ratio for PPO-clip objective.\n",
       "- train_iters (int): How many steps to take over the latest data batch.\n",
       "- batch_size (int): How many interactions to collect per update.\n",
       "- pol_lr (float): Learning rate for the policy optimizer.\n",
       "- val_lr (float): Learning rate for the value optimizer.\n",
       "- maxkl (float): Max allowed KL divergence between policy updates.\n",
       "- seed (int): Random seed for pytorch and numpy\n",
       "- evaluate (bool): Whether to run eval episodes at the end of each epoch. Saves episodes using gym.wrappers.Monitor.\n",
       "- monitor_dir (str): Directory for monitor to write to. Default is /tmp"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example training PPO on the [CartPole-v1 environment](https://gym.openai.com/envs/CartPole-v1/). Since it is a [PyTorch-Lightning Module](https://pytorch-lightning.readthedocs.io/en/latest/) it is trained using their Trainer API. \n",
    "\n",
    "**Note that this PPO implementation needs to be more thoroughly benchmarked and so may be a work in progress.**\n",
    "\n",
    "The `reload_dataloaders_every_epoch` flag is needed to ensure that during each training step, the updates are computed on the latest batch of data.\n",
    "\n",
    "To see how we implement this, view the source code for the `PPO` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(\"CartPole-v1\")\n",
    "trainer = pl.Trainer(reload_dataloaders_every_epoch=True, max_epochs=25)\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_datasets.ipynb.\n",
      "Converted 02_buffers.ipynb.\n",
      "Converted 03_neuralnets.ipynb.\n",
      "Converted 04_losses.ipynb.\n",
      "Converted 05_env_wrappers.ipynb.\n",
      "Converted 06_loops.ipynb.\n",
      "Converted 07_algorithms.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/jfpettit/rl-bolts-benchmark\" target=\"_blank\">https://app.wandb.ai/jfpettit/rl-bolts-benchmark</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/jfpettit/rl-bolts-benchmark/runs/rmcvf5tf\" target=\"_blank\">https://app.wandb.ai/jfpettit/rl-bolts-benchmark/runs/rmcvf5tf</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "/Users/jacobpettit/anaconda3/envs/flare/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jacobpettit/anaconda3/envs/flare/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Tried to pass invalid video frame, marking as broken: Your frame has data type int64, but we require uint8 (i.e. RGB values from 0-255).\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/Users/jacobpettit/anaconda3/envs/flare/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Tried to pass invalid video frame, marking as broken: Your frame has data type int64, but we require uint8 (i.e. RGB values from 0-255).\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Tried to reset environment which is not done. While the monitor is active for AntBulletEnv-v0, you cannot call reset() unless the episode is over.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-260c19937963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mexpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rl-bolts-benchmark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_gym\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWandbLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreload_dataloaders_every_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-35bbb935c60d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, hidden_sizes, gamma, lam, clipratio, train_iters, batch_size, pol_lr, val_lr, maxkl, seed, evaluate, monitor_dir)\u001b[0m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-35bbb935c60d>\u001b[0m in \u001b[0;36minner_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolgrad_interaction_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracker_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/rl_bolts/rl_bolts/loops.py\u001b[0m in \u001b[0;36mpolgrad_interaction_loop\u001b[0;34m(env, agent, buffer, num_interactions, horizon)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mlens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     infos = {\n",
      "\u001b[0;32m~/Documents/rl_bolts/rl_bolts/env_wrappers.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mtensor_obs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mof\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \"\"\"\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtensor_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flare/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flare/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flare/lib/python3.7/site-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for AntBulletEnv-v0, you cannot call reset() unless the episode is over."
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import pybullet_envs\n",
    "env_name = \"AntBulletEnv-v0\"\n",
    "logger_name = \"ppo-ant-seed0\"\n",
    "expt = wandb.init(name=logger_name, project=\"rl-bolts-benchmark\", monitor_gym=True)\n",
    "logger = pl.loggers.WandbLogger(experiment=expt)\n",
    "agent = PPO(env_name, monitor_dir=logger_name, evaluate=False)\n",
    "logger.watch(agent)\n",
    "trainer = pl.Trainer(reload_dataloaders_every_epoch=True, max_epochs=250, logger=logger)\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
