# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_neuralnets.ipynb (unless otherwise specified).

__all__ = ['MLP', 'Actor', 'CategoricalPolicy', 'GaussianPolicy', 'ActorCritic']

# Cell
import numpy as np
from collections import namedtuple
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy import signal
import gym
from scipy.signal import lfilter
from typing import Optional, Iterable, List, Dict, Callable, Union, Tuple

# Cell
class MLP(nn.Module):
    r"""
    A class for building a simple MLP network.

    Args:
    - layer_sizes (list or tuple): Layer sizes for the network.
    - activations (Function): Activation function for MLP net.
    - out_act (Function): Output activation function
    - out_squeeze (bool): Whether to squeeze the output of the network.
    """

    def __init__(
        self,
        layer_sizes: Union[List, Tuple],
        activations: Optional[Callable] = torch.tanh,
        out_act: Optional[bool] = None,
        out_squeeze: Optional[bool] = False,
    ):
        super(MLP, self).__init__()
        self.layers = nn.ModuleList()
        self.activations = activations
        self.out_act = out_act
        self.out_squeeze = out_squeeze

        for i, l in enumerate(layer_sizes[1:]):
            self.layers.append(nn.Linear(layer_sizes[i], l))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass for MLP network"""
        for l in self.layers[:-1]:
            x = self.activations(l(x))

        if self.out_act is None:
            x = self.layers[-1](x)
        else:
            x = self.out_act(self.layers[-1](x))

        return torch.squeeze(x, -1) if self.out_squeeze else x

# Cell
class Actor(nn.Module):
    """
    Barebones class structure for an Actor.
    """
    def action_distribution(self, states):
        raise NotImplementedError

    def logprob_from_distribution(self, policy, action):
        raise NotImplementedError

    def forward(self, x, a = None):
        """
        Forward pass for an policy.

        Args:
        - x (torch.Tensor): Input state from the environment.
        - a (torch.Tensor): Action that was taken.

        Returns:
        - policy (PyTorch distribution): The policy distribution.
        - logp_a (torch.Tensor): Log-probability of input action under the policy distribution.
        """
        policy = self.action_distribution(x)
        logp_a = None
        if a is not None:
            logp_a = self.logprob_from_distribution(policy, a)
        return policy, logp_a

# Cell
class CategoricalPolicy(Actor):
    r"""
    A class for a Categorical Policy network. Used in discrete action space environments.

    The policy is an `MLP`.

    Args:
    - state_features (int): Dimensionality of the state space.
    - action_dim (int): Dimensionality of the action space.
    - hidden_sizes (list or tuple): Hidden layer sizes.
    - activation (Function): Activation function for the network.
    - out_activation (Function): Output activation function for the network.
    """

    def __init__(
        self,
        state_features: int,
        action_dim: int,
        hidden_sizes: Union[List, Tuple],
        activation: Callable,
        out_activation: Callable,
    ):
        super().__init__()
        self.net = MLP(
            [state_features] + list(hidden_sizes) + [action_dim], activations=activation
        )

    def action_distribution(self, x: torch.Tensor):
        """
        Defines action distribution conditioned on input state.

        Args:
        - x(torch.Tensor): input state

        Returns:
        - Categorical distribution: Policy over the action space.
        """
        logits = self.net(x)
        return torch.distributions.Categorical(logits=logits)

    def logprob_from_distribution(self, policy: torch.distributions.Distribution, actions: torch.Tensor):
        """
        Calculate the log-probability of an action under a policy.

        Args:
        - policy (torch.distributions.Distribution): The policy distribution over input state.
        - actions (torch.Tensor): Actions to take log probability of.

        Returns:
        - log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution.
        """
        return policy.log_prob(actions)

# Cell
class GaussianPolicy(Actor):
    r"""
    A class for a Gaussian Policy network. Used in continuous action space environments. The policy is an :func:`~MLP`.

    Args:
    - state_features (int): Dimensionality of the state space.
    - action_dim (int): Dimensionality of the action space.
    - hidden_sizes (list or tuple): Hidden layer sizes.
    - activation (Function): Activation function for the network.
    - out_activation (Function): Output activation function for the network.
    """

    def __init__(
        self,
        state_features: int,
        action_dim: int,
        hidden_sizes: Union[List, Tuple],
        activation: Callable,
        out_activation: Callable,
    ):
        super().__init__()

        self.net = MLP(
            [state_features] + list(hidden_sizes) + [action_dim],
            activations=activation,
            out_act=out_activation,
        )

        self.logstd = nn.Parameter(-0.5 * torch.ones(action_dim, dtype=torch.float32))

    def action_distribution(self, states):
        """
        Defines action distribution conditioned on input state.

        Args:
        - x(torch.Tensor): input state

        Returns:
        - Normal distribution: Policy over the action space.
        """
        mus = self.net(states)
        std = torch.exp(self.logstd)
        return torch.distributions.Normal(mus, std)

    def logprob_from_distribution(self, policy, actions):
        """
        Calculate the log-probability of an action under a policy.

        Args:
        - policy (torch.distributions.Distribution): The policy distribution over input state.
        - actions (torch.Tensor): Actions to take log probability of.

        Returns:
        - log_probs (torch.Tensor): Log-probabilities of actions under the policy distribution.
        """
        return policy.log_prob(actions).sum(axis=-1)

# Cell
class ActorCritic(nn.Module):
    r"""
    An Actor Critic class for Policy Gradient algorithms.

    Has built-in capability to work with continuous (gym.spaces.Box) and discrete (gym.spaces.Discrete) action spaces.
    The policy and value function are both `MLP`.

    If working with a different action space,
    the user can pass in a custom policy class for that action space as an argument.

    Args:
    - state_features (int): Dimensionality of the state space.
    - action_space (gym.spaces.Space): Action space of the environment.
    - hidden_sizes (list or tuple): Hidden layer sizes.
    - activation (Function): Activation function for the network.
    - out_activation (Function): Output activation function for the network.
    - policy (nn.Module): Custom policy class for an environment where the action space is not gym.spaces.Box or gym.spaces.Discrete

    """

    def __init__(
        self,
        state_features: int,
        action_space: int,
        hidden_sizes: Optional[Union[Tuple, List]] = (32, 32),
        activation: Optional[Callable] = torch.tanh,
        out_activation: Optional[Callable] = None,
        policy: Optional[nn.Module] = None,
    ):
        super(ActorCritic, self).__init__()

        obs_dim = state_features

        if isinstance(action_space, gym.spaces.Discrete):
            act_dim = action_space.n
            pol = CategoricalPolicy

        elif isinstance(action_space, gym.spaces.Box):
            act_dim = action_space.shape[0]
            pol = GaussianPolicy
        else:
            act_dim = action_space
            pol = policy

        self.policy = pol(
            obs_dim,
            act_dim,
            hidden_sizes,
            activation,
            out_activation
        )

        self.value_f = MLP(
            [state_features] + list(hidden_sizes) + [1],
            activations=activation,
            out_squeeze=True,
        )

    def step(self, x: torch.Tensor):
        """
        Get action, action log probability, and value estimate for an input state.

        Args:
        - x (torch.Tensor): input state.

        Returns:
        - action (np.array): Action chosen by the policy.
        - logp_action (np.array): Log probability of that action chosen by the policy.
        - value (np.array): Value estimate of the current state.
        """
        with torch.no_grad():
            policy = self.policy.action_distribution(x)
            action = policy.sample()
            logp_action = self.policy.logprob_from_distribution(policy, action)
            value = self.value_f(x)
        return action.numpy(), logp_action.numpy(), value.numpy()

    def act(self, x: torch.Tensor):
        """
        Similar to `step`, but get only the action.

        Args:
        - x (torch.Tensor): input state

        Returns:
        - action (np.array): Action chosen by the policy.
        """
        return self.step(x)[0]